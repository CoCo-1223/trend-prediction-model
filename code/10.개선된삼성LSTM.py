"""
ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ - 10.ê°œì„ ëœì‚¼ì„±LSTM.py
ìƒì„±ì¼: 2025-06-08
íŒ€: í˜„ì¢…ë¯¼(íŒ€ì¥), ì‹ ì˜ˆì›(íŒ€ì›), ê¹€ì±„ì€(íŒ€ì›)

9ë²ˆ ì½”ë“œì—ì„œ ìƒì„±ëœ 7ì¼ í‰ê·  ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬
Samsung ì¤‘ì‹¬ì˜ ê³ ë„í™”ëœ LSTM ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

ëª©í‘œ: RÂ² > 0.3 (ê¸°ì¡´ -0.19ì—ì„œ ëŒ€í­ ê°œì„ )
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
from sklearn.preprocessing import RobustScaler
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ê²°ê³¼ë¬¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
RESULTS_BASE = "/Users/jm/Desktop/ì¶©ë¶ëŒ€í•™êµ/ì¶©ëŒ€ 4í•™ë…„ 1í•™ê¸°/2. ë¹…ë°ì´í„°ì´í•´ì™€ë¶„ì„/íŒ€í”„ë¡œì íŠ¸/trend-prediction-model/results/2025-0608"
PROJECT_BASE = "/Users/jm/Desktop/ì¶©ë¶ëŒ€í•™êµ/ì¶©ëŒ€ 4í•™ë…„ 1í•™ê¸°/2. ë¹…ë°ì´í„°ì´í•´ì™€ë¶„ì„/íŒ€í”„ë¡œì íŠ¸/trend-prediction-model"

# 9ë²ˆ ì½”ë“œì—ì„œ ìƒì„±ëœ ë°ì´í„° ê²½ë¡œ
LSTM_SEQUENCES_PATH = f"{RESULTS_BASE}/data/features/lstm_training_sequences.pkl"
FEATURE_INFO_PATH = f"{RESULTS_BASE}/data/features/feature_info.json"
WEEKLY_FEATURES_PATH = f"{RESULTS_BASE}/data/features/weekly_sentiment_features.csv"

# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)
plt.rcParams['font.family'] = ['Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def setup_directories():
    """ê²°ê³¼ë¬¼ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
    directories = [
        f"{RESULTS_BASE}/models/trained",
        f"{RESULTS_BASE}/models/evaluation",
        f"{RESULTS_BASE}/models/features_importance",
        f"{RESULTS_BASE}/models/predictions",
        f"{RESULTS_BASE}/visualizations/model_performance",
        f"{RESULTS_BASE}/reports/technical"
    ]
    for dir_path in directories:
        os.makedirs(dir_path, exist_ok=True)
    print("âœ… ëª¨ë¸ ê´€ë ¨ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ")

# ë””ë ‰í† ë¦¬ ìë™ ìƒì„±
setup_directories()

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

class SamsungLSTMDataset(Dataset):
    """Samsung LSTM í•™ìŠµìš© ë°ì´í„°ì…‹"""
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class AdvancedSamsungLSTM(nn.Module):
    """ê³ ë„í™”ëœ Samsung LSTM ëª¨ë¸"""
    def __init__(self, input_size=58, hidden_size=128, num_layers=3, dropout=0.3):
        super(AdvancedSamsungLSTM, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # ì–‘ë°©í–¥ LSTM
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bidirectional=True,
            dropout=dropout,
            batch_first=True
        )
        
        # ë©€í‹°í—¤ë“œ ì–´í…ì…˜
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,
            num_heads=8,
            dropout=0.2,
            batch_first=True
        )
        
        # Layer Normalization
        self.layer_norm = nn.LayerNorm(hidden_size * 2)
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1)
        )
        
    def forward(self, x):
        # LSTM í†µê³¼
        lstm_out, _ = self.lstm(x)
        
        # Self-Attention ì ìš©
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # Layer Normalization
        normalized = self.layer_norm(attn_out)
        
        # ë§ˆì§€ë§‰ ì‹œê°„ ìŠ¤í…ì˜ ì¶œë ¥ ì‚¬ìš©
        final_output = normalized[:, -1, :]
        
        # ìµœì¢… ì˜ˆì¸¡
        prediction = self.fc(final_output)
        
        return prediction

def load_preprocessed_data():
    """9ë²ˆì—ì„œ ìƒì„±ëœ ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë”©"""
    print("ğŸ“ 9ë²ˆ ì½”ë“œì—ì„œ ìƒì„±ëœ ì „ì²˜ë¦¬ ë°ì´í„° ë¡œë”© ì¤‘...")
    
    try:
        with open(LSTM_SEQUENCES_PATH, 'rb') as f:
            lstm_data = pickle.load(f)
        
        print("ğŸ” ì‹¤ì œ ë°ì´í„° êµ¬ì¡° í™•ì¸ ì¤‘...")
        print(f"   - ë¡œë”©ëœ í‚¤ë“¤: {list(lstm_data.keys())}")
        
        X = lstm_data['X']
        y = lstm_data['y']
        split_data = lstm_data.get('split_data', None)
        scalers = lstm_data.get('scalers', None)
        
        print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        print(f"   - ì „ì²´ ì‹œí€€ìŠ¤ ìˆ˜: {X.shape[0]}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {X.shape[1]}")
        print(f"   - íŠ¹ì„± ê°œìˆ˜: {X.shape[2]}")
        
        if 'company_info' in lstm_data:
            company_info = lstm_data['company_info']
            print(f"   - Samsung ìƒ˜í”Œ ìˆ˜: {company_info['Samsung']['count']}")
            print(f"   - Apple ìƒ˜í”Œ ìˆ˜: {company_info['Apple']['count']}")
        else:
            print("   - company_info ì—†ìŒ. ë©”íƒ€ë°ì´í„°ì—ì„œ íšŒì‚¬ ì •ë³´ ì¶”ì¶œ ì˜ˆì •")
            company_info = None
        
        return X, y, split_data, scalers, company_info
        
    except FileNotFoundError:
        print("âŒ 9ë²ˆ ì½”ë“œ ê²°ê³¼ë¬¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 9ë²ˆ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return None, None, None, None, None
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None, None, None

def filter_samsung_data(X, y, company_info=None):
    """Samsung ë°ì´í„°ë§Œ ì¶”ì¶œ"""
    print("ğŸ” Samsung ë°ì´í„° í•„í„°ë§ ì¤‘...")
    
    if company_info is not None and 'Samsung' in company_info:
        samsung_indices = company_info['Samsung']['indices']
        X_samsung = X[samsung_indices]
        y_samsung = y[samsung_indices]
    else:
        try:
            weekly_features_path = f"{RESULTS_BASE}/data/features/weekly_sentiment_features.csv"
            if os.path.exists(weekly_features_path):
                weekly_df = pd.read_csv(weekly_features_path)
                samsung_mask = weekly_df['Company'] == 'Samsung'
                samsung_indices = samsung_mask[samsung_mask].index.tolist()
                X_samsung = X[samsung_indices]
                y_samsung = y[samsung_indices]
                print(f"   - ë©”íƒ€ë°ì´í„°ì—ì„œ Samsung ì¸ë±ìŠ¤ ì¶”ì¶œ: {len(samsung_indices)}ê°œ")
            else:
                print("   - ë©”íƒ€ë°ì´í„° ì—†ìŒ. ì „ì²´ ë°ì´í„°ì˜ í›„ë°˜ë¶€ë¥¼ Samsungìœ¼ë¡œ ê°€ì •")
                mid_point = len(X) // 2
                X_samsung = X[mid_point:]
                y_samsung = y[mid_point:]
        except Exception as e:
            print(f"   - ë©”íƒ€ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            mid_point = len(X) // 2
            X_samsung = X[mid_point:]
            y_samsung = y[mid_point:]
    
    print(f"âœ… Samsung ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {len(X_samsung)}ê°œ ì‹œí€€ìŠ¤")
    return X_samsung, y_samsung

def create_samsung_train_val_test_split(X_samsung, y_samsung, train_ratio=0.6, val_ratio=0.2):
    """Samsung ë°ì´í„° ì‹œê°„ ìˆœì„œ ìœ ì§€í•˜ë©° ë¶„í• """
    print("ğŸ“Š Samsung ë°ì´í„° ë¶„í•  ì¤‘...")
    
    n_samples = len(X_samsung)
    train_size = int(n_samples * train_ratio)
    val_size = int(n_samples * val_ratio)
    
    X_train = X_samsung[:train_size]
    y_train = y_samsung[:train_size]
    
    X_val = X_samsung[train_size:train_size + val_size]
    y_val = y_samsung[train_size:train_size + val_size]
    
    X_test = X_samsung[train_size + val_size:]
    y_test = y_samsung[train_size + val_size:]
    
    print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ")
    print(f"   - í›ˆë ¨: {len(X_train)} ({len(X_train)/n_samples*100:.1f}%)")
    print(f"   - ê²€ì¦: {len(X_val)} ({len(X_val)/n_samples*100:.1f}%)")
    print(f"   - í…ŒìŠ¤íŠ¸: {len(X_test)} ({len(X_test)/n_samples*100:.1f}%)")
    
    return X_train, y_train, X_val, y_val, X_test, y_test

def comprehensive_evaluation(y_true, y_pred):
    """ë‹¤ì°¨ì› í‰ê°€ ì§€í‘œ"""
    
    def direction_accuracy(y_true, y_pred):
        true_direction = np.diff(y_true) > 0
        pred_direction = np.diff(y_pred) > 0
        return np.mean(true_direction == pred_direction) * 100
    
    def threshold_accuracy(y_true, y_pred, threshold=0.1):
        return np.mean(np.abs(y_true - y_pred) < threshold) * 100
    
    metrics = {
        'r2_score': r2_score(y_true, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
        'mae': mean_absolute_error(y_true, y_pred),
        'mape': mean_absolute_percentage_error(y_true, y_pred),
        'direction_accuracy': direction_accuracy(y_true, y_pred),
        'threshold_accuracy': threshold_accuracy(y_true, y_pred, threshold=0.1)
    }
    
    return metrics

class ModelTrainer:
    """LSTM ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model, device):
        self.model = model.to(device)
        self.device = device
        self.training_history = {'train_loss': [], 'val_loss': []}
        self.best_val_loss = float('inf')
        
    def train_epoch(self, train_loader, optimizer, criterion):
        self.model.train()
        total_loss = 0
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
            
            optimizer.zero_grad()
            outputs = self.model(batch_x)
            loss = criterion(outputs.squeeze(), batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(train_loader)
    
    def validate_epoch(self, val_loader, criterion):
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                outputs = self.model(batch_x)
                loss = criterion(outputs.squeeze(), batch_y)
                total_loss += loss.item()
        
        return total_loss / len(val_loader)
    
    def train(self, train_loader, val_loader, num_epochs=200, learning_rate=0.001, patience=20):
        print("ğŸš€ Samsung LSTM ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)
        criterion = nn.MSELoss()
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
        
        early_stopping_counter = 0
        
        for epoch in range(num_epochs):
            train_loss = self.train_epoch(train_loader, optimizer, criterion)
            val_loss = self.validate_epoch(val_loader, criterion)
            scheduler.step(val_loss)
            
            self.training_history['train_loss'].append(train_loss)
            self.training_history['val_loss'].append(val_loss)
            
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                torch.save(self.model.state_dict(), 
                          f"{RESULTS_BASE}/models/trained/best_samsung_lstm_weekly.pth")
                early_stopping_counter = 0
            else:
                early_stopping_counter += 1
            
            if (epoch + 1) % 20 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                print(f"Epoch [{epoch+1}/{num_epochs}] - "
                      f"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, "
                      f"LR: {current_lr:.2e}")
            
            if early_stopping_counter >= patience:
                print(f"â° ì¡°ê¸° ì¢…ë£Œ: {patience}íšŒ ì—°ì† ê°œì„  ì—†ìŒ (Epoch {epoch+1})")
                break
        
        self.model.load_state_dict(torch.load(f"{RESULTS_BASE}/models/trained/best_samsung_lstm_weekly.pth"))
        print("âœ… í›ˆë ¨ ì™„ë£Œ ë° ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ")
        
        return self.training_history
    
    def predict(self, test_loader):
        self.model.eval()
        predictions = []
        actuals = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                outputs = self.model(batch_x)
                predictions.extend(outputs.squeeze().cpu().numpy())
                actuals.extend(batch_y.cpu().numpy())
        
        return np.array(actuals), np.array(predictions)

def create_comprehensive_visualizations(y_true, y_pred, training_history):
    """ì¢…í•© ì‹œê°í™” ìƒì„± (9ê°œ ì„œë¸Œí”Œë¡¯)"""
    print("ğŸ“Š ì¢…í•© ì‹œê°í™” ìƒì„± ì¤‘...")
    
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    fig.suptitle('Samsung LSTM Model - Comprehensive Analysis', fontsize=20, y=0.98)
    
    # 1. ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
    ax1 = axes[0, 0]
    ax1.plot(y_true, label='Actual', color='blue', alpha=0.7)
    ax1.plot(y_pred, label='Predicted', color='red', alpha=0.7)
    ax1.set_title('Time Series Prediction Results')
    ax1.set_xlabel('Time')
    ax1.set_ylabel('Sentiment Score')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„
    ax2 = axes[0, 1]
    ax2.scatter(y_true, y_pred, alpha=0.6, color='purple')
    ax2.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
    ax2.set_title('Predicted vs Actual')
    ax2.set_xlabel('Actual')
    ax2.set_ylabel('Predicted')
    ax2.grid(True, alpha=0.3)
    
    r2 = r2_score(y_true, y_pred)
    ax2.text(0.05, 0.95, f'RÂ² = {r2:.4f}', transform=ax2.transAxes, 
             bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.5))
    
    # 3. í›ˆë ¨ ê³¡ì„ 
    ax3 = axes[0, 2]
    ax3.plot(training_history['train_loss'], label='Train Loss', color='blue')
    ax3.plot(training_history['val_loss'], label='Validation Loss', color='orange')
    ax3.set_title('Training Curves')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Loss')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ì˜¤ì°¨ ë¶„í¬
    ax4 = axes[1, 0]
    errors = y_pred - y_true
    ax4.hist(errors, bins=30, alpha=0.7, color='green', edgecolor='black')
    ax4.set_title('Error Distribution')
    ax4.set_xlabel('Prediction Error')
    ax4.set_ylabel('Frequency')
    ax4.axvline(0, color='red', linestyle='--', alpha=0.8)
    ax4.grid(True, alpha=0.3)
    
    # 5. ì”ì°¨ í”Œë¡¯
    ax5 = axes[1, 1]
    ax5.scatter(y_pred, errors, alpha=0.6, color='orange')
    ax5.axhline(0, color='red', linestyle='--', alpha=0.8)
    ax5.set_title('Residual Plot')
    ax5.set_xlabel('Predicted Values')
    ax5.set_ylabel('Residuals')
    ax5.grid(True, alpha=0.3)
    
    # 6. ë°©í–¥ì„± ì •í™•ë„
    ax6 = axes[1, 2]
    true_direction = np.diff(y_true) > 0
    pred_direction = np.diff(y_pred) > 0
    direction_acc = np.mean(true_direction == pred_direction) * 100
    
    categories = ['Correct', 'Incorrect']
    correct_count = np.sum(true_direction == pred_direction)
    incorrect_count = len(true_direction) - correct_count
    values = [correct_count, incorrect_count]
    
    ax6.pie(values, labels=categories, autopct='%1.1f%%', startangle=90)
    ax6.set_title(f'Direction Accuracy: {direction_acc:.1f}%')
    
    # 7. ì„±ëŠ¥ ì§€í‘œ ìš”ì•½
    ax7 = axes[2, 0]
    ax7.axis('off')
    
    metrics = comprehensive_evaluation(y_true, y_pred)
    metrics_text = f"""
    Performance Metrics:
    
    RÂ² Score: {metrics['r2_score']:.4f}
    RMSE: {metrics['rmse']:.4f}
    MAE: {metrics['mae']:.4f}
    MAPE: {metrics['mape']:.2f}%
    Direction Accuracy: {metrics['direction_accuracy']:.1f}%
    Threshold Accuracy: {metrics['threshold_accuracy']:.1f}%
    """
    
    ax7.text(0.1, 0.9, metrics_text, transform=ax7.transAxes, fontsize=12,
             verticalalignment='top', bbox=dict(boxstyle="round", facecolor='lightblue', alpha=0.8))
    
    # 8. ìµœê·¼ 30ì¼ ì˜ˆì¸¡ vs ì‹¤ì œ
    ax8 = axes[2, 1]
    last_30_true = y_true[-30:]
    last_30_pred = y_pred[-30:]
    
    ax8.plot(range(30), last_30_true, 'o-', label='Actual', color='blue', markersize=4)
    ax8.plot(range(30), last_30_pred, 's-', label='Predicted', color='red', markersize=4)
    ax8.set_title('Last 30 Days - Detailed View')
    ax8.set_xlabel('Days')
    ax8.set_ylabel('Sentiment Score')
    ax8.legend()
    ax8.grid(True, alpha=0.3)
    
    # 9. ì˜ˆì¸¡ ì‹ ë¢°êµ¬ê°„
    ax9 = axes[2, 2]
    
    error_std = np.std(errors)
    upper_bound = y_pred + 1.96 * error_std
    lower_bound = y_pred - 1.96 * error_std
    
    recent_range = range(len(y_true) - 50, len(y_true))
    ax9.plot(recent_range, y_true[-50:], 'o-', label='Actual', color='blue', markersize=3)
    ax9.plot(recent_range, y_pred[-50:], 's-', label='Predicted', color='red', markersize=3)
    ax9.fill_between(recent_range, upper_bound[-50:], lower_bound[-50:], 
                     alpha=0.3, color='gray', label='95% Confidence')
    ax9.set_title('Prediction with Confidence Interval')
    ax9.set_xlabel('Time Index')
    ax9.set_ylabel('Sentiment Score')
    ax9.legend()
    ax9.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.93)
    
    viz_path = f"{RESULTS_BASE}/visualizations/model_performance/samsung_lstm_comprehensive_analysis.png"
    plt.savefig(viz_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… ì¢…í•© ì‹œê°í™” ì €ì¥: {viz_path}")
    
    plt.show()
    
    return metrics

def predict_future_30_days(model, last_sequence, device):
    """30ì¼ ë¯¸ë˜ ì˜ˆì¸¡"""
    print("ğŸ”® 30ì¼ ë¯¸ë˜ ì˜ˆì¸¡ ìƒì„± ì¤‘...")
    
    model.eval()
    predictions = []
    current_sequence = last_sequence.copy()
    
    with torch.no_grad():
        for day in range(30):
            input_tensor = torch.FloatTensor(current_sequence).unsqueeze(0).to(device)
            pred = model(input_tensor)
            pred_value = pred.cpu().numpy()[0, 0]
            
            predictions.append(pred_value)
            
            new_features = current_sequence[-1].copy()
            new_features[0] = pred_value
            
            current_sequence = np.roll(current_sequence, -1, axis=0)
            current_sequence[-1] = new_features
    
    return np.array(predictions)

def create_future_prediction_chart(future_predictions, y_true, y_pred):
    """ë¯¸ë˜ ì˜ˆì¸¡ ì°¨íŠ¸ ìƒì„±"""
    print("ğŸ“ˆ ë¯¸ë˜ ì˜ˆì¸¡ ì°¨íŠ¸ ìƒì„± ì¤‘...")
    
    plt.figure(figsize=(15, 8))
    
    past_range = range(-60, 0)
    plt.plot(past_range, y_true[-60:], 'o-', label='Actual (Past)', color='blue', markersize=3)
    plt.plot(past_range, y_pred[-60:], 's-', label='Predicted (Past)', color='red', markersize=3)
    
    future_range = range(0, 30)
    plt.plot(future_range, future_predictions, '^-', label='Future Prediction', 
             color='green', markersize=4, linewidth=2)
    
    past_error_std = np.std(y_pred - y_true)
    upper_bound = future_predictions + 1.96 * past_error_std
    lower_bound = future_predictions - 1.96 * past_error_std
    
    plt.fill_between(future_range, upper_bound, lower_bound, 
                     alpha=0.3, color='green', label='95% Confidence Interval')
    
    plt.axvline(0, color='black', linestyle='--', alpha=0.7, label='Current Time')
    
    plt.title('Samsung Sentiment Score - 30 Days Future Prediction', fontsize=16)
    plt.xlabel('Days (Relative to Current Time)')
    plt.ylabel('Sentiment Score')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    stats_text = f"""
    Future Prediction Summary:
    Mean: {np.mean(future_predictions):.3f}
    Trend: {(future_predictions[-1] - future_predictions[0]):.3f}
    Volatility: {np.std(future_predictions):.3f}
    """
    
    plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, 
             verticalalignment='top', bbox=dict(boxstyle="round", facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    
    future_path = f"{RESULTS_BASE}/visualizations/model_performance/samsung_future_prediction_30days.png"
    plt.savefig(future_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… ë¯¸ë˜ ì˜ˆì¸¡ ì°¨íŠ¸ ì €ì¥: {future_path}")
    
    plt.show()

def save_model_results(model, metrics, training_history, future_predictions, y_true, y_pred, input_size):
    """ëª¨ë¸ ê²°ê³¼ ì €ì¥"""
    print("ğŸ’¾ ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    model_config = {
        'model_type': 'AdvancedSamsungLSTM',
        'input_size': input_size,
        'hidden_size': 128,
        'num_layers': 3,
        'dropout': 0.3,
        'attention_heads': 8,
        'training_epochs': len(training_history['train_loss']),
        'best_val_loss': min(training_history['val_loss']),
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open(f"{RESULTS_BASE}/models/evaluation/samsung_model_config.json", 'w') as f:
        json.dump(model_config, f, indent=2)
    
    with open(f"{RESULTS_BASE}/models/evaluation/model_performance_metrics.json", 'w') as f:
        json.dump(metrics, f, indent=2)
    
    history_df = pd.DataFrame(training_history)
    history_df.to_csv(f"{RESULTS_BASE}/models/evaluation/training_history.csv", index=False)
    
    predictions_df = pd.DataFrame({
        'actual': y_true,
        'predicted': y_pred,
        'error': y_pred - y_true,
        'abs_error': np.abs(y_pred - y_true)
    })
    predictions_df.to_csv(f"{RESULTS_BASE}/models/predictions/test_predictions.csv", index=False)
    
    future_dates = [datetime.now() + timedelta(days=i) for i in range(1, 31)]
    future_df = pd.DataFrame({
        'date': future_dates,
        'predicted_sentiment': future_predictions,
        'prediction_day': range(1, 31)
    })
    future_df.to_csv(f"{RESULTS_BASE}/models/predictions/30day_future_predictions.csv", index=False)
    
    report = f"""# Samsung LSTM Model Performance Report

## Model Configuration
- Model Type: Advanced Samsung LSTM with Attention
- Input Features: {input_size} dimensions
- Sequence Length: 4 weeks
- Hidden Size: 128
- LSTM Layers: 3 (Bidirectional)
- Attention Heads: 8
- Training Epochs: {len(training_history['train_loss'])}

## Performance Metrics
- RÂ² Score: {metrics['r2_score']:.4f}
- RMSE: {metrics['rmse']:.4f}
- MAE: {metrics['mae']:.4f}
- MAPE: {metrics['mape']:.2f}%
- Direction Accuracy: {metrics['direction_accuracy']:.1f}%
- Threshold Accuracy (Â±0.1): {metrics['threshold_accuracy']:.1f}%

## Model Analysis
- Best Validation Loss: {min(training_history['val_loss']):.6f}
- Training Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- Future Prediction Trend: {(future_predictions[-1] - future_predictions[0]):.3f}

## Business Insights
- Model Performance Level: {'Excellent' if metrics['r2_score'] > 0.5 else 'Good' if metrics['r2_score'] > 0.3 else 'Moderate' if metrics['r2_score'] > 0.1 else 'Needs Improvement'}
- Practical Usage: {'Highly Recommended' if metrics['direction_accuracy'] > 70 else 'Recommended' if metrics['direction_accuracy'] > 60 else 'Conditional Use'}
- Risk Level: {'Low' if metrics['rmse'] < 0.3 else 'Medium' if metrics['rmse'] < 0.5 else 'High'}
"""
    
    with open(f"{RESULTS_BASE}/reports/technical/samsung_lstm_performance_report.md", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
    print(f"   - ëª¨ë¸ ì„¤ì •: models/evaluation/samsung_model_config.json")
    print(f"   - ì„±ëŠ¥ ì§€í‘œ: models/evaluation/model_performance_metrics.json")
    print(f"   - í›ˆë ¨ ì´ë ¥: models/evaluation/training_history.csv")
    print(f"   - ì˜ˆì¸¡ ê²°ê³¼: models/predictions/test_predictions.csv")
    print(f"   - ë¯¸ë˜ ì˜ˆì¸¡: models/predictions/30day_future_predictions.csv")
    print(f"   - ì„±ëŠ¥ ë¦¬í¬íŠ¸: reports/technical/samsung_lstm_performance_report.md")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Samsung LSTM ëª¨ë¸ ê°œë°œ ì‹œì‘!")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë”©
    X, y, split_data, scalers, company_info = load_preprocessed_data()
    
    if X is None:
        print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 2. Samsung ë°ì´í„° í•„í„°ë§
    X_samsung, y_samsung = filter_samsung_data(X, y, company_info)
    
    # 3. ë°ì´í„° ë¶„í• 
    X_train, y_train, X_val, y_val, X_test, y_test = create_samsung_train_val_test_split(
        X_samsung, y_samsung, train_ratio=0.6, val_ratio=0.2
    )
    
    # 4. ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
    print("ğŸ“¦ ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
    train_dataset = SamsungLSTMDataset(X_train, y_train)
    val_dataset = SamsungLSTMDataset(X_val, y_val)
    test_dataset = SamsungLSTMDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # 5. ëª¨ë¸ ìƒì„±
    print("ğŸ§  ê³ ë„í™”ëœ Samsung LSTM ëª¨ë¸ ìƒì„± ì¤‘...")
    input_size = X_train.shape[2]
    model = AdvancedSamsungLSTM(
        input_size=input_size,
        hidden_size=128,
        num_layers=3,
        dropout=0.3
    )
    
    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    print(f"ğŸ“Š ì‹¤ì œ ì…ë ¥ íŠ¹ì„± ê°œìˆ˜: {input_size}")
    
    # 6. ëª¨ë¸ í›ˆë ¨
    trainer = ModelTrainer(model, device)
    training_history = trainer.train(
        train_loader, val_loader, 
        num_epochs=200, 
        learning_rate=0.001, 
        patience=20
    )
    
    # 7. ëª¨ë¸ í‰ê°€
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    y_true, y_pred = trainer.predict(test_loader)
    
    # 8. ì¢…í•© ì‹œê°í™”
    metrics = create_comprehensive_visualizations(y_true, y_pred, training_history)
    
    # 9. ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ¯ ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼")
    print("=" * 60)
    print(f"RÂ² Score: {metrics['r2_score']:.4f}")
    print(f"RMSE: {metrics['rmse']:.4f}")
    print(f"MAE: {metrics['mae']:.4f}")
    print(f"MAPE: {metrics['mape']:.2f}%")
    print(f"Direction Accuracy: {metrics['direction_accuracy']:.1f}%")
    print(f"Threshold Accuracy: {metrics['threshold_accuracy']:.1f}%")
    
    # ì„±ëŠ¥ í‰ê°€
    if metrics['r2_score'] > 0.5:
        print("ğŸ† ìš°ìˆ˜í•œ ì„±ëŠ¥! ì‹¤ë¬´ í™œìš© ê°•ë ¥ ì¶”ì²œ")
    elif metrics['r2_score'] > 0.3:
        print("âœ… ì¢‹ì€ ì„±ëŠ¥! ì‹¤ë¬´ í™œìš© ê°€ëŠ¥")
    elif metrics['r2_score'] > 0.1:
        print("âš ï¸ ë³´í†µ ì„±ëŠ¥. ì¶”ê°€ ê°œì„  í•„ìš”")
    else:
        print("âŒ ì„±ëŠ¥ ë¶€ì¡±. ëª¨ë¸ ì¬ì„¤ê³„ í•„ìš”")
    
    # 10. ë¯¸ë˜ ì˜ˆì¸¡
    print("\nğŸ”® 30ì¼ ë¯¸ë˜ ì˜ˆì¸¡ ìƒì„± ì¤‘...")
    last_sequence = X_test[-1]
    future_predictions = predict_future_30_days(model, last_sequence, device)
    
    # 11. ë¯¸ë˜ ì˜ˆì¸¡ ì‹œê°í™”
    create_future_prediction_chart(future_predictions, y_true, y_pred)
    
    # 12. ëª¨ë“  ê²°ê³¼ ì €ì¥
    save_model_results(model, metrics, training_history, future_predictions, y_true, y_pred, input_size)
    
    # 13. ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸŠ Samsung LSTM ëª¨ë¸ ê°œë°œ ì™„ë£Œ!")
    print("=" * 60)
    print("ğŸ“ ìƒì„±ëœ ê²°ê³¼ë¬¼:")
    print(f"   - í›ˆë ¨ëœ ëª¨ë¸: models/trained/best_samsung_lstm_weekly.pth")
    print(f"   - ì¢…í•© ë¶„ì„ ì°¨íŠ¸: visualizations/model_performance/samsung_lstm_comprehensive_analysis.png")
    print(f"   - ë¯¸ë˜ ì˜ˆì¸¡ ì°¨íŠ¸: visualizations/model_performance/samsung_future_prediction_30days.png")
    print(f"   - ì„±ëŠ¥ ë¦¬í¬íŠ¸: reports/technical/samsung_lstm_performance_report.md")
    print(f"   - ì˜ˆì¸¡ ê²°ê³¼: models/predictions/test_predictions.csv")
    print(f"   - ë¯¸ë˜ ì˜ˆì¸¡: models/predictions/30day_future_predictions.csv")
    
    print("\nâœ¨ ë‹¤ìŒ ë‹¨ê³„: 11ë²ˆ ì œí’ˆì¶œì‹œì„íŒ©íŠ¸ë¶„ì„.py ì‹¤í–‰")
    
    return {
        'model': model,
        'metrics': metrics,
        'predictions': {
            'y_true': y_true,
            'y_pred': y_pred,
            'future': future_predictions
        },
        'training_history': training_history
    }

if __name__ == "__main__":
    results = main()