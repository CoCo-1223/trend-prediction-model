"""
ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ê¸°ë°˜ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ - 7ì¼ í‰ê·  LSTM ë°ì´í„° ì „ì²˜ë¦¬
ìƒì„±ì¼: 2025-06-08
íŒ€: í˜„ì¢…ë¯¼(íŒ€ì¥), ì‹ ì˜ˆì›(íŒ€ì›), ê¹€ì±„ì€(íŒ€ì›)

ëª©í‘œ:
- 8ë²ˆ ì½”ë“œì—ì„œ ìƒì„±ëœ í†µí•© ë°ì´í„°ë¥¼ í™œìš©í•œ ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
- ì£¼ê°„ ë°ì´í„° ê¸°ë°˜ LSTM ì‹œí€€ìŠ¤ ìƒì„±
- íšŒì‚¬ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì°¨ë³„í™”ëœ íŠ¹ì„± ìƒì„±
- SHAP ë¶„ì„ì„ ìœ„í•œ í•´ì„ ê°€ëŠ¥í•œ íŠ¹ì„± ì„¤ê³„
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import pickle
import json
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# ê²°ê³¼ë¬¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
RESULTS_BASE = "/Users/jm/Desktop/ì¶©ë¶ëŒ€í•™êµ/ì¶©ëŒ€ 4í•™ë…„ 1í•™ê¸°/2. ë¹…ë°ì´í„°ì´í•´ì™€ë¶„ì„/íŒ€í”„ë¡œì íŠ¸/trend-prediction-model/results/2025-0608"
PROJECT_BASE = "/Users/jm/Desktop/ì¶©ë¶ëŒ€í•™êµ/ì¶©ëŒ€ 4í•™ë…„ 1í•™ê¸°/2. ë¹…ë°ì´í„°ì´í•´ì™€ë¶„ì„/íŒ€í”„ë¡œì íŠ¸/trend-prediction-model"

# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)
plt.rcParams['font.family'] = ['Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def setup_directories():
    """ê²°ê³¼ë¬¼ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
    directories = [
        f"{RESULTS_BASE}/visualizations/weekly_analysis",
        f"{RESULTS_BASE}/visualizations/impact_analysis", 
        f"{RESULTS_BASE}/visualizations/model_performance",
        f"{RESULTS_BASE}/visualizations/final_insights",
        f"{RESULTS_BASE}/data/processed",
        f"{RESULTS_BASE}/data/features",
        f"{RESULTS_BASE}/data/predictions", 
        f"{RESULTS_BASE}/data/exports",
        f"{RESULTS_BASE}/models/trained",
        f"{RESULTS_BASE}/models/evaluation",
        f"{RESULTS_BASE}/models/features_importance",
        f"{RESULTS_BASE}/models/predictions",
        f"{RESULTS_BASE}/reports/technical",
        f"{RESULTS_BASE}/reports/business",
        f"{RESULTS_BASE}/reports/methodology",
        f"{RESULTS_BASE}/reports/final"
    ]
    for dir_path in directories:
        os.makedirs(dir_path, exist_ok=True)
    print(f"âœ… ê²°ê³¼ë¬¼ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ: {RESULTS_BASE}")

# ì‹¤í–‰ ì‹œì‘ ì‹œ ë””ë ‰í† ë¦¬ ìë™ ìƒì„±
setup_directories()

class WeeklyLSTMDataProcessor:
    """7ì¼ í‰ê·  LSTM ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"""
    
    def __init__(self):
        self.results_base = RESULTS_BASE
        self.project_base = PROJECT_BASE
        
        # ë°ì´í„° ê²½ë¡œ ì„¤ì •
        self.weekly_data_path = f"{self.results_base}/data/processed/weekly_sentiment_stock_data.csv"
        self.product_data_path = f"{self.results_base}/data/processed/combined_product_timeline.csv"
        self.sentiment_data_path = f"{self.project_base}/data/processed"
        self.stock_data_path = f"{self.project_base}/stock"
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”
        self.scalers = {}
        
        # ë¡œê·¸ ê¸°ë¡
        self.processing_log = []
        
        print("ğŸ“Š Weekly LSTM Data Processor ì´ˆê¸°í™” ì™„ë£Œ")
        
    def log_process(self, message):
        """ì²˜ë¦¬ ê³¼ì • ë¡œê¹…"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.processing_log.append(log_entry)
        print(log_entry)
    
    def load_base_data(self):
        """8ë²ˆ ì½”ë“œì—ì„œ ìƒì„±ëœ í†µí•© ë°ì´í„° ë¡œë”©"""
        self.log_process("ê¸°ë³¸ ë°ì´í„° ë¡œë”© ì‹œì‘")
        
        try:
            # í†µí•© ì£¼ê°„ ë°ì´í„° ë¡œë”©
            self.weekly_data = pd.read_csv(self.weekly_data_path)
            self.weekly_data['Date'] = pd.to_datetime(self.weekly_data['Date'])
            
            # ì œí’ˆ ì¶œì‹œ ë°ì´í„° ë¡œë”©
            self.product_data = pd.read_csv(self.product_data_path)
            self.product_data['Date'] = pd.to_datetime(self.product_data['Date'])
            
            self.log_process(f"í†µí•© ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.weekly_data):,}ê±´")
            self.log_process(f"ì œí’ˆ ì¶œì‹œ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.product_data):,}ê±´")
            
            # ë°ì´í„° ê¸°ë³¸ ì •ë³´ ì¶œë ¥
            print("\nğŸ“ˆ í†µí•© ë°ì´í„° ê¸°ë³¸ ì •ë³´:")
            print(f"- ì „ì²´ ë°ì´í„°: {len(self.weekly_data):,}ê±´")
            print(f"- Apple ë°ì´í„°: {len(self.weekly_data[self.weekly_data['Company'] == 'Apple']):,}ê±´")
            print(f"- Samsung ë°ì´í„°: {len(self.weekly_data[self.weekly_data['Company'] == 'Samsung']):,}ê±´")
            print(f"- ê¸°ê°„: {self.weekly_data['Date'].min()} ~ {self.weekly_data['Date'].max()}")
            
            return True
            
        except Exception as e:
            self.log_process(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            return False
    
    def create_time_features(self, df):
        """ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„±"""
        self.log_process("ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„± ì‹œì‘")
        
        # ê¸°ë³¸ ì‹œê°„ íŠ¹ì„±
        df['Year'] = df['Date'].dt.year
        df['Month'] = df['Date'].dt.month
        df['Quarter'] = df['Date'].dt.quarter
        df['DayOfYear'] = df['Date'].dt.dayofyear
        df['WeekOfYear'] = df['Date'].dt.isocalendar().week
        
        # ê³„ì ˆì„± íŠ¹ì„±
        df['is_quarter_end'] = df['Month'].isin([3, 6, 9, 12]).astype(int)
        df['is_year_end'] = (df['Month'] == 12).astype(int)
        df['is_apple_season'] = df['Month'].isin([9, 10, 11]).astype(int)  # ì•„ì´í° ì¶œì‹œ ì‹œì¦Œ
        
        # ì£¼ê¸°ì  íŠ¹ì„± (ì‚¬ì¸/ì½”ì‚¬ì¸ ì¸ì½”ë”©)
        df['month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)
        df['quarter_sin'] = np.sin(2 * np.pi * df['Quarter'] / 4)
        df['quarter_cos'] = np.cos(2 * np.pi * df['Quarter'] / 4)
        
        self.log_process("ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„± ì™„ë£Œ (12ê°œ)")
        return df
    
    def create_sentiment_features(self, df):
        """ê°ì„± ê´€ë ¨ ê³ ê¸‰ íŠ¹ì„± ìƒì„±"""
        self.log_process("ê°ì„± íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘")
        
        # íšŒì‚¬ë³„ë¡œ ì²˜ë¦¬
        for company in df['Company'].unique():
            mask = df['Company'] == company
            company_data = df[mask].copy()
            
            # 1. ê°ì„± ëª¨ë©˜í…€ íŠ¹ì„± (7ì¼, 14ì¼, 30ì¼)
            df.loc[mask, 'sentiment_momentum_7d'] = company_data['sentiment_score_7d_avg'].pct_change(periods=1)
            df.loc[mask, 'sentiment_momentum_14d'] = company_data['sentiment_score_7d_avg'].pct_change(periods=2)
            df.loc[mask, 'sentiment_momentum_30d'] = company_data['sentiment_score_7d_avg'].pct_change(periods=4)
            
            # 2. ê°ì„± ë³€ë™ì„± (7ì¼, 14ì¼ ë¡¤ë§ í‘œì¤€í¸ì°¨)
            df.loc[mask, 'sentiment_volatility_7d'] = company_data['sentiment_score_7d_avg'].rolling(window=2, min_periods=1).std()
            df.loc[mask, 'sentiment_volatility_14d'] = company_data['sentiment_score_7d_avg'].rolling(window=3, min_periods=1).std()
            
            # 3. ê°ì„± Z-ìŠ¤ì½”ì–´ (ì—°ë„ë³„ ì •ê·œí™”)
            for year in company_data['Year'].unique():
                year_mask = (df['Company'] == company) & (df['Year'] == year)
                year_data = df[year_mask]['sentiment_score_7d_avg']
                if len(year_data) > 1:
                    df.loc[year_mask, 'sentiment_zscore'] = stats.zscore(year_data, nan_policy='omit')
            
            # 4. ê°ì„± íŠ¸ë Œë“œ (ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°)
            df.loc[mask, 'sentiment_trend_7d'] = company_data['sentiment_score_7d_avg'].rolling(window=2, min_periods=2).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 2 else 0
            )
            
            # 5. ê°ì„± ìƒëŒ€ í¬ì§€ì…˜ (ìµœê·¼ 4ì£¼ ë‚´ ë°±ë¶„ìœ„)
            df.loc[mask, 'sentiment_percentile_4w'] = company_data['sentiment_score_7d_avg'].rolling(window=4, min_periods=1).rank(pct=True)
            
            # 6. ê°ì„± ì•ˆì •ì„± (ë³€ë™ê³„ìˆ˜)
            rolling_mean = company_data['sentiment_score_7d_avg'].rolling(window=4, min_periods=1).mean()
            rolling_std = company_data['sentiment_score_7d_avg'].rolling(window=4, min_periods=1).std()
            df.loc[mask, 'sentiment_stability'] = rolling_std / (rolling_mean + 1e-6)  # ë³€ë™ê³„ìˆ˜
        
        # 7. ë‰´ìŠ¤ ë³¼ë¥¨ íŠ¹ì„±
        df['news_volume_momentum'] = df.groupby('Company')['news_count'].pct_change()
        df['news_volume_spike'] = (df['news_count'] > df.groupby('Company')['news_count'].transform(lambda x: x.quantile(0.9))).astype(int)
        
        self.log_process("ê°ì„± íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ (13ê°œ)")
        return df
    
    def create_stock_features(self, df):
        """ì£¼ê°€ ê´€ë ¨ ê³ ê¸‰ íŠ¹ì„± ìƒì„±"""
        self.log_process("ì£¼ê°€ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘")
        
        # íšŒì‚¬ë³„ë¡œ ì²˜ë¦¬
        for company in df['Company'].unique():
            mask = df['Company'] == company
            company_data = df[mask].copy()
            
            # 1. ì£¼ê°€ ëª¨ë©˜í…€ íŠ¹ì„±
            df.loc[mask, 'stock_momentum_7d'] = company_data['stock_price_7d_avg'].pct_change(periods=1)
            df.loc[mask, 'stock_momentum_14d'] = company_data['stock_price_7d_avg'].pct_change(periods=2)
            df.loc[mask, 'stock_momentum_30d'] = company_data['stock_price_7d_avg'].pct_change(periods=4)
            
            # 2. ì£¼ê°€ ë³€ë™ì„±
            df.loc[mask, 'stock_volatility_7d'] = company_data['stock_price_7d_avg'].rolling(window=2, min_periods=1).std()
            df.loc[mask, 'stock_volatility_14d'] = company_data['stock_price_7d_avg'].rolling(window=3, min_periods=1).std()
            
            # 3. ì£¼ê°€ Z-ìŠ¤ì½”ì–´ (ì—°ë„ë³„)
            for year in company_data['Year'].unique():
                year_mask = (df['Company'] == company) & (df['Year'] == year)
                year_data = df[year_mask]['stock_price_7d_avg']
                if len(year_data) > 1:
                    df.loc[year_mask, 'stock_zscore'] = stats.zscore(year_data, nan_policy='omit')
            
            # 4. ì£¼ê°€ ìƒëŒ€ ê°•ë„ (4ì£¼ ë‚´ ë°±ë¶„ìœ„)
            df.loc[mask, 'stock_rsi_4w'] = company_data['stock_price_7d_avg'].rolling(window=4, min_periods=1).rank(pct=True)
            
            # 5. ì´ë™í‰ê·  êµì°¨ ì‹ í˜¸
            ma_short = company_data['stock_price_7d_avg'].rolling(window=2, min_periods=1).mean()
            ma_long = company_data['stock_price_7d_avg'].rolling(window=4, min_periods=1).mean()
            df.loc[mask, 'ma_cross_signal'] = (ma_short > ma_long).astype(int)
            
            # 6. ì£¼ê°€ vs ê¸°ì¤€ í¸ì°¨ (ì—°ë„ë³„ í‰ê·  ëŒ€ë¹„)
            year_avg = company_data.groupby('Year')['stock_price_7d_avg'].transform('mean')
            df.loc[mask, 'stock_vs_year_avg'] = (company_data['stock_price_7d_avg'] - year_avg) / year_avg
        
        self.log_process("ì£¼ê°€ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ (11ê°œ)")
        return df
    
    def create_product_launch_features(self, df):
        """ì œí’ˆ ì¶œì‹œ ê´€ë ¨ íŠ¹ì„± ìƒì„±"""
        self.log_process("ì œí’ˆ ì¶œì‹œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘")
        
        # íšŒì‚¬ë³„ë¡œ ì²˜ë¦¬
        for company in df['Company'].unique():
            mask = df['Company'] == company
            company_data = df[mask].copy()
            company_products = self.product_data[self.product_data['Company'] == company]
            
            # ê° ë‚ ì§œì— ëŒ€í•´ ì œí’ˆ ì¶œì‹œ ê´€ë ¨ íŠ¹ì„± ê³„ì‚°
            days_to_next = []
            days_since_last = []
            launch_impact_scores = []
            launch_categories = []
            launch_counts = []
            
            for date in company_data['Date']:
                # ë‹¤ìŒ ì œí’ˆ ì¶œì‹œê¹Œì§€ì˜ ì¼ìˆ˜
                future_launches = company_products[company_products['Date'] > date]
                if len(future_launches) > 0:
                    next_launch = future_launches['Date'].min()
                    days_to_next.append((next_launch - date).days)
                else:
                    days_to_next.append(365)  # ê¸°ë³¸ê°’
                
                # ë§ˆì§€ë§‰ ì œí’ˆ ì¶œì‹œ ì´í›„ ì¼ìˆ˜
                past_launches = company_products[company_products['Date'] <= date]
                if len(past_launches) > 0:
                    last_launch = past_launches['Date'].max()
                    days_since_last.append((date - last_launch).days)
                else:
                    days_since_last.append(365)  # ê¸°ë³¸ê°’
                
                # ì„íŒ©íŠ¸ ìŠ¤ì½”ì–´ (ê°€ê¹Œìš´ ì¶œì‹œì¼ë“¤ì˜ ê°€ì¤‘ í‰ê· )
                nearby_launches = company_products[
                    (company_products['Date'] >= date - timedelta(days=60)) &
                    (company_products['Date'] <= date + timedelta(days=60))
                ]
                
                if len(nearby_launches) > 0:
                    # ê±°ë¦¬ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
                    distances = abs((nearby_launches['Date'] - date).dt.days)
                    weights = np.exp(-distances / 30)  # 30ì¼ ê°ì‡  í•¨ìˆ˜
                    impact_score = weights.sum()
                    
                    # ì£¼ìš” ì œí’ˆ ì¹´í…Œê³ ë¦¬ í™•ì¸
                    if company == 'Apple':
                        if any('iPhone' in prod for prod in nearby_launches['Product']):
                            category = 'iPhone'
                        elif any('iPad' in prod for prod in nearby_launches['Product']):
                            category = 'iPad'
                        else:
                            category = 'Other'
                    else:  # Samsung
                        if any('Galaxy S' in prod for prod in nearby_launches['Product']):
                            category = 'Galaxy_S'
                        elif any('Galaxy Note' in prod or 'Galaxy Z' in prod for prod in nearby_launches['Product']):
                            category = 'Galaxy_Premium'
                        else:
                            category = 'Other'
                    
                    # í•´ë‹¹ ê¸°ê°„ ì¶œì‹œ ì œí’ˆ ìˆ˜
                    launch_count = len(nearby_launches)
                else:
                    impact_score = 0
                    category = 'None'
                    launch_count = 0
                
                launch_impact_scores.append(impact_score)
                launch_categories.append(category)
                launch_counts.append(launch_count)
            
            # íŠ¹ì„± í• ë‹¹
            df.loc[mask, 'days_to_next_launch'] = days_to_next
            df.loc[mask, 'days_since_last_launch'] = days_since_last
            df.loc[mask, 'launch_impact_score'] = launch_impact_scores
            df.loc[mask, 'launch_category'] = launch_categories
            df.loc[mask, 'launch_count_nearby'] = launch_counts
            
            # ì¶”ê°€ íŒŒìƒ íŠ¹ì„±
            df.loc[mask, 'launch_proximity'] = 1 / (1 + np.minimum(df.loc[mask, 'days_to_next_launch'], 
                                                                  df.loc[mask, 'days_since_last_launch']))
            df.loc[mask, 'pre_launch_period'] = (df.loc[mask, 'days_to_next_launch'] <= 30).astype(int)
            df.loc[mask, 'post_launch_period'] = (df.loc[mask, 'days_since_last_launch'] <= 30).astype(int)
        
        # ì¹´í…Œê³ ë¦¬ ì¸ì½”ë”©
        le = LabelEncoder()
        df['launch_category_encoded'] = le.fit_transform(df['launch_category'])
        
        self.log_process("ì œí’ˆ ì¶œì‹œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ (9ê°œ)")
        return df
    
    def create_interaction_features(self, df):
        """ìƒí˜¸ì‘ìš© ë° ë¹„ìœ¨ íŠ¹ì„± ìƒì„±"""
        self.log_process("ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì‹œì‘")
        
        # 1. ê°ì„±-ì£¼ê°€ ìƒí˜¸ì‘ìš©
        df['sentiment_stock_ratio'] = df['sentiment_score_7d_avg'] / (df['stock_price_7d_avg'] + 1e-6)
        df['sentiment_stock_product'] = df['sentiment_score_7d_avg'] * df['stock_momentum_7d']
        
        # 2. ê¸°ì¤€ì„  ëŒ€ë¹„ í¸ì°¨
        for company in df['Company'].unique():
            mask = df['Company'] == company
            
            # ì—°ë„ë³„ ê¸°ì¤€ì„ 
            year_sentiment_avg = df[mask].groupby('Year')['sentiment_score_7d_avg'].transform('mean')
            year_stock_avg = df[mask].groupby('Year')['stock_price_7d_avg'].transform('mean')
            
            df.loc[mask, 'sentiment_vs_baseline'] = (df.loc[mask, 'sentiment_score_7d_avg'] - year_sentiment_avg) / year_sentiment_avg
            df.loc[mask, 'stock_vs_baseline'] = (df.loc[mask, 'stock_price_7d_avg'] - year_stock_avg) / year_stock_avg
            
            # ì „ì²´ ê¸°ì¤€ì„  ëŒ€ë¹„
            overall_sentiment_avg = df[mask]['sentiment_score_7d_avg'].mean()
            overall_stock_avg = df[mask]['stock_price_7d_avg'].mean()
            
            df.loc[mask, 'sentiment_vs_overall'] = (df.loc[mask, 'sentiment_score_7d_avg'] - overall_sentiment_avg) / overall_sentiment_avg
            df.loc[mask, 'stock_vs_overall'] = (df.loc[mask, 'stock_price_7d_avg'] - overall_stock_avg) / overall_stock_avg
        
        # 3. ë‰´ìŠ¤ ë³¼ë¥¨ ìƒí˜¸ì‘ìš©
        df['news_sentiment_interaction'] = df['news_count'] * df['sentiment_score_7d_avg']
        df['news_launch_interaction'] = df['news_count'] * df['launch_impact_score']
        
        # 4. ì‹œê°„-ì´ë²¤íŠ¸ ìƒí˜¸ì‘ìš©
        df['quarter_launch_interaction'] = df['Quarter'] * df['launch_impact_score']
        df['season_sentiment_interaction'] = df['is_apple_season'] * df['sentiment_score_7d_avg']
        
        self.log_process("ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ì™„ë£Œ (10ê°œ)")
        return df
    
    def create_lag_features(self, df):
        """ì‹œì°¨ íŠ¹ì„± ìƒì„± (ê°ì„±ì´ ì£¼ê°€ë¥¼ ì„ í–‰í•˜ëŠ” íŒ¨í„´ ë°˜ì˜)"""
        self.log_process("ì‹œì°¨ íŠ¹ì„± ìƒì„± ì‹œì‘")
        
        # íšŒì‚¬ë³„ë¡œ ì²˜ë¦¬
        for company in df['Company'].unique():
            mask = df['Company'] == company
            company_data = df[mask].copy().sort_values('Date')
            
            # ê°ì„± ì„ í–‰ ì§€í‘œ (1-3ì£¼ ì „ ê°ì„±)
            df.loc[mask, 'sentiment_lag_1w'] = company_data['sentiment_score_7d_avg'].shift(1)
            df.loc[mask, 'sentiment_lag_2w'] = company_data['sentiment_score_7d_avg'].shift(2)
            df.loc[mask, 'sentiment_lag_3w'] = company_data['sentiment_score_7d_avg'].shift(3)
            
            # ê°ì„± ë³€í™” ì„ í–‰ ì§€í‘œ
            df.loc[mask, 'sentiment_momentum_lag_1w'] = company_data['sentiment_momentum_7d'].shift(1)
            df.loc[mask, 'sentiment_momentum_lag_2w'] = company_data['sentiment_momentum_7d'].shift(2)
            
            # ì£¼ê°€ ì„ í–‰ ì§€í‘œ (ë‹¤ìŒ ì£¼ ì˜ˆì¸¡ì„ ìœ„í•œ í˜„ì¬/ê³¼ê±° ì£¼ê°€)
            df.loc[mask, 'stock_lag_1w'] = company_data['stock_price_7d_avg'].shift(1)
            df.loc[mask, 'stock_lag_2w'] = company_data['stock_price_7d_avg'].shift(2)
            
            # í˜¼í•© ì„ í–‰ ì§€í‘œ
            df.loc[mask, 'sentiment_stock_lag_interaction'] = (
                company_data['sentiment_score_7d_avg'].shift(1) * 
                company_data['stock_momentum_7d'].shift(1)
            )
        
        self.log_process("ì‹œì°¨ íŠ¹ì„± ìƒì„± ì™„ë£Œ (8ê°œ)")
        return df
    
    def create_all_features(self):
        """ëª¨ë“  íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰"""
        self.log_process("=== ì „ì²´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘ ===")
        
        # ê¸°ë³¸ ë°ì´í„° ë¡œë”©
        if not self.load_base_data():
            return None
        
        # ì „ì²´ ë°ì´í„° ë³µì‚¬
        df = self.weekly_data.copy()
        
        # ê° íŠ¹ì„± ê·¸ë£¹ ìƒì„±
        df = self.create_time_features(df)
        df = self.create_sentiment_features(df)
        df = self.create_stock_features(df)
        df = self.create_product_launch_features(df)
        df = self.create_interaction_features(df)
        df = self.create_lag_features(df)
        
        # ë¬´í•œê°’ ë° NaN ì²˜ë¦¬
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬ (íšŒì‚¬ë³„ë¡œ)
        for company in df['Company'].unique():
            mask = df['Company'] == company
            df.loc[mask] = df.loc[mask].fillna(method='ffill').fillna(method='bfill').fillna(0)
        
        self.feature_df = df
        
        # ìµœì¢… íŠ¹ì„± ëª©ë¡ ì •ë¦¬
        feature_columns = [col for col in df.columns if col not in ['Date', 'Company', 'Year']]
        numeric_features = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()
        
        self.log_process(f"ì „ì²´ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {len(numeric_features)}ê°œ ìˆ«ìí˜• íŠ¹ì„±")
        self.log_process(f"ì „ì²´ ë°ì´í„° í¬ê¸°: {df.shape}")
        
        return df
    
    def create_lstm_sequences(self, df, sequence_length=4, target_col='sentiment_score_7d_avg'):
        """LSTMìš© ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        self.log_process(f"LSTM ì‹œí€€ìŠ¤ ìƒì„± ì‹œì‘ (ê¸¸ì´: {sequence_length}ì£¼)")
        
        # íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ (ìˆ«ìí˜•ë§Œ)
        feature_columns = [col for col in df.columns if col not in ['Date', 'Company', 'Year', 'launch_category']]
        numeric_features = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()
        
        # íšŒì‚¬ë³„ë¡œ ì‹œí€€ìŠ¤ ìƒì„±
        all_sequences = []
        all_targets = []
        all_dates = []
        all_companies = []
        
        for company in df['Company'].unique():
            company_data = df[df['Company'] == company].sort_values('Date').reset_index(drop=True)
            company_features = company_data[numeric_features].values
            company_targets = company_data[target_col].values
            
            # ë°ì´í„° ì •ê·œí™” (íšŒì‚¬ë³„ ìŠ¤ì¼€ì¼ëŸ¬)
            scaler = RobustScaler()
            company_features_scaled = scaler.fit_transform(company_features)
            self.scalers[company] = scaler
            
            # ì‹œí€€ìŠ¤ ìƒì„±
            for i in range(sequence_length, len(company_data)):
                # ì…ë ¥ ì‹œí€€ìŠ¤ (ê³¼ê±° 4ì£¼)
                sequence = company_features_scaled[i-sequence_length:i]
                
                # íƒ€ê²Ÿ (í˜„ì¬ ì£¼ì˜ ê°ì„± ì ìˆ˜)
                target = company_targets[i]
                
                # ë©”íƒ€ ì •ë³´
                date = company_data.iloc[i]['Date']
                
                all_sequences.append(sequence)
                all_targets.append(target)
                all_dates.append(date)
                all_companies.append(company)
        
        # numpy ë°°ì—´ë¡œ ë³€í™˜
        X = np.array(all_sequences)
        y = np.array(all_targets)
        
        # ë©”íƒ€ ì •ë³´ DataFrame
        meta_df = pd.DataFrame({
            'Date': all_dates,
            'Company': all_companies,
            'Target': y
        })
        
        self.log_process(f"ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {X.shape[0]}ê°œ ì‹œí€€ìŠ¤, ì…ë ¥ ì°¨ì›: {X.shape[1:]} â†’ ì¶œë ¥: {y.shape}")
        
        # íšŒì‚¬ë³„ ë¶„í¬ í™•ì¸
        for company in meta_df['Company'].unique():
            count = (meta_df['Company'] == company).sum()
            self.log_process(f"- {company}: {count:,}ê°œ ì‹œí€€ìŠ¤")
        
        return X, y, meta_df, numeric_features
    
    def create_validation_split(self, X, y, meta_df, test_size=0.2, val_size=0.2):
        """ì‹œê³„ì—´ íŠ¹ì„±ì„ ê³ ë ¤í•œ ê²€ì¦ ë°ì´í„° ë¶„í• """
        self.log_process("ê²€ì¦ ë°ì´í„° ë¶„í•  ì‹œì‘")
        
        # ë‚ ì§œ ê¸°ì¤€ ì •ë ¬
        sort_idx = meta_df['Date'].argsort()
        X_sorted = X[sort_idx]
        y_sorted = y[sort_idx]
        meta_sorted = meta_df.iloc[sort_idx].reset_index(drop=True)
        
        # ì‹œê°„ ìˆœì„œ ê¸°ì¤€ ë¶„í• 
        n_total = len(X_sorted)
        n_train = int(n_total * (1 - test_size - val_size))
        n_val = int(n_total * val_size)
        
        # ë¶„í•  ì¸ë±ìŠ¤
        train_idx = slice(0, n_train)
        val_idx = slice(n_train, n_train + n_val)
        test_idx = slice(n_train + n_val, n_total)
        
        # ë°ì´í„° ë¶„í• 
        X_train = X_sorted[train_idx]
        X_val = X_sorted[val_idx]
        X_test = X_sorted[test_idx]
        
        y_train = y_sorted[train_idx]
        y_val = y_sorted[val_idx]
        y_test = y_sorted[test_idx]
        
        meta_train = meta_sorted.iloc[train_idx]
        meta_val = meta_sorted.iloc[val_idx]
        meta_test = meta_sorted.iloc[test_idx]
        
        self.log_process(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        self.log_process(f"- í›ˆë ¨: {len(X_train):,}ê°œ ({train_idx.start}-{train_idx.stop})")
        self.log_process(f"- ê²€ì¦: {len(X_val):,}ê°œ ({val_idx.start}-{val_idx.stop})")
        self.log_process(f"- í…ŒìŠ¤íŠ¸: {len(X_test):,}ê°œ ({test_idx.start}-{test_idx.stop})")
        self.log_process(f"- í›ˆë ¨ ê¸°ê°„: {meta_train['Date'].min()} ~ {meta_train['Date'].max()}")
        self.log_process(f"- ê²€ì¦ ê¸°ê°„: {meta_val['Date'].min()} ~ {meta_val['Date'].max()}")
        self.log_process(f"- í…ŒìŠ¤íŠ¸ ê¸°ê°„: {meta_test['Date'].min()} ~ {meta_test['Date'].max()}")
        
        return {
            'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
            'y_train': y_train, 'y_val': y_val, 'y_test': y_test,
            'meta_train': meta_train, 'meta_val': meta_val, 'meta_test': meta_test
        }
    
    def analyze_feature_importance_preparation(self, df, numeric_features):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ì¤€ë¹„"""
        self.log_process("íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ì¤€ë¹„ ì‹œì‘")
        
        # íŠ¹ì„± ê·¸ë£¹ ì •ì˜
        feature_groups = {
            'Time_Features': [col for col in numeric_features if any(x in col.lower() for x in ['month', 'quarter', 'year', 'day', 'season'])],
            'Sentiment_Features': [col for col in numeric_features if 'sentiment' in col.lower()],
            'Stock_Features': [col for col in numeric_features if 'stock' in col.lower()],
            'Product_Launch_Features': [col for col in numeric_features if any(x in col.lower() for x in ['launch', 'days_to', 'days_since'])],
            'Interaction_Features': [col for col in numeric_features if any(x in col.lower() for x in ['ratio', 'product', 'interaction', 'vs_'])],
            'Lag_Features': [col for col in numeric_features if 'lag' in col.lower()],
            'News_Features': [col for col in numeric_features if 'news' in col.lower()],
            'Momentum_Features': [col for col in numeric_features if 'momentum' in col.lower()],
            'Volatility_Features': [col for col in numeric_features if 'volatility' in col.lower()]
        }
        
        # ê·¸ë£¹ë³„ íŠ¹ì„± ê°œìˆ˜ ë¡œê¹…
        for group, features in feature_groups.items():
            self.log_process(f"- {group}: {len(features)}ê°œ")
        
        return feature_groups
    
    def create_correlation_analysis(self, df, numeric_features):
        """ìƒê´€ê´€ê³„ ë¶„ì„ ë° ì‹œê°í™”"""
        self.log_process("ìƒê´€ê´€ê³„ ë¶„ì„ ì‹œì‘")
        
        # í•µì‹¬ íŠ¹ì„±ë“¤ë§Œ ì„ ë³„ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°í™”ê°€ ì–´ë ¤ì›€)
        key_features = [
            'sentiment_score_7d_avg', 'stock_price_7d_avg', 
            'sentiment_momentum_7d', 'stock_momentum_7d',
            'sentiment_volatility_7d', 'stock_volatility_7d',
            'launch_impact_score', 'days_to_next_launch',
            'sentiment_stock_ratio', 'news_count',
            'sentiment_vs_baseline', 'stock_vs_baseline',
            'sentiment_lag_1w', 'sentiment_lag_2w'
        ]
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŠ¹ì„±ë§Œ ì„ íƒ
        available_features = [f for f in key_features if f in numeric_features]
        
        # íšŒì‚¬ë³„ ìƒê´€ê´€ê³„ ê³„ì‚°
        correlation_results = {}
        
        for company in df['Company'].unique():
            company_data = df[df['Company'] == company][available_features]
            corr_matrix = company_data.corr()
            correlation_results[company] = corr_matrix
        
        # ì „ì²´ ìƒê´€ê´€ê³„ (íšŒì‚¬ êµ¬ë¶„ ì—†ì´)
        overall_corr = df[available_features].corr()
        correlation_results['Overall'] = overall_corr
        
        # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ìƒì„±
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        fig.suptitle('Feature Correlation Analysis', fontsize=16, y=0.98)
        
        # Apple ìƒê´€ê´€ê³„
        sns.heatmap(correlation_results['Apple'], annot=True, cmap='RdBu_r', center=0,
                   ax=axes[0,0], cbar_kws={'shrink': .8})
        axes[0,0].set_title('Apple - Feature Correlations')
        axes[0,0].tick_params(axis='x', rotation=45)
        axes[0,0].tick_params(axis='y', rotation=0)
        
        # Samsung ìƒê´€ê´€ê³„
        sns.heatmap(correlation_results['Samsung'], annot=True, cmap='RdBu_r', center=0,
                   ax=axes[0,1], cbar_kws={'shrink': .8})
        axes[0,1].set_title('Samsung - Feature Correlations')
        axes[0,1].tick_params(axis='x', rotation=45)
        axes[0,1].tick_params(axis='y', rotation=0)
        
        # ì „ì²´ ìƒê´€ê´€ê³„
        sns.heatmap(correlation_results['Overall'], annot=True, cmap='RdBu_r', center=0,
                   ax=axes[1,0], cbar_kws={'shrink': .8})
        axes[1,0].set_title('Overall - Feature Correlations')
        axes[1,0].tick_params(axis='x', rotation=45)
        axes[1,0].tick_params(axis='y', rotation=0)
        
        # ê°ì„±-ì£¼ê°€ ì‹œì°¨ ìƒê´€ê´€ê³„ (ë³„ë„ ë¶„ì„)
        lag_features = ['sentiment_score_7d_avg', 'sentiment_lag_1w', 'sentiment_lag_2w', 'sentiment_lag_3w']
        stock_feature = 'stock_price_7d_avg'
        
        lag_corr_data = []
        for company in df['Company'].unique():
            company_data = df[df['Company'] == company]
            for lag_feat in lag_features:
                if lag_feat in company_data.columns:
                    corr = company_data[lag_feat].corr(company_data[stock_feature])
                    lag_name = lag_feat.replace('sentiment_', '').replace('_7d_avg', '_current')
                    lag_corr_data.append({'Company': company, 'Lag_Feature': lag_name, 'Correlation': corr})
        
        lag_corr_df = pd.DataFrame(lag_corr_data)
        if not lag_corr_df.empty:
            lag_pivot = lag_corr_df.pivot(index='Lag_Feature', columns='Company', values='Correlation')
            sns.heatmap(lag_pivot, annot=True, cmap='RdBu_r', center=0,
                       ax=axes[1,1], cbar_kws={'shrink': .8})
            axes[1,1].set_title('Sentiment-Stock Lag Correlations')
            axes[1,1].tick_params(axis='x', rotation=0)
            axes[1,1].tick_params(axis='y', rotation=0)
        
        plt.tight_layout()
        
        # ì €ì¥
        correlation_path = f"{self.results_base}/visualizations/model_performance/feature_correlation_matrix.png"
        plt.savefig(correlation_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        self.log_process(f"ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ ë° ì €ì¥: {correlation_path}")
        
        return correlation_results
    
    def create_feature_statistics(self, df, numeric_features):
        """íŠ¹ì„± í†µê³„ ì •ë³´ ìƒì„±"""
        self.log_process("íŠ¹ì„± í†µê³„ ì •ë³´ ìƒì„± ì‹œì‘")
        
        # ì „ì²´ í†µê³„
        overall_stats = df[numeric_features].describe()
        
        # íšŒì‚¬ë³„ í†µê³„
        company_stats = {}
        for company in df['Company'].unique():
            company_data = df[df['Company'] == company][numeric_features]
            company_stats[company] = company_data.describe()
        
        # íŠ¹ì„±ë³„ ë³€ë™ê³„ìˆ˜ (CV) ê³„ì‚°
        cv_stats = {}
        for company in df['Company'].unique():
            company_data = df[df['Company'] == company][numeric_features]
            cv = company_data.std() / (company_data.mean() + 1e-6)
            cv_stats[company] = cv
        
        # ê²°ê³¼ ì €ì¥
        stats_results = {
            'overall_statistics': overall_stats,
            'company_statistics': company_stats,
            'coefficient_of_variation': cv_stats
        }
        
        # CSVë¡œ ì €ì¥
        overall_stats.to_csv(f"{self.results_base}/data/features/overall_feature_statistics.csv")
        
        for company, stats in company_stats.items():
            stats.to_csv(f"{self.results_base}/data/features/{company.lower()}_feature_statistics.csv")
        
        self.log_process("íŠ¹ì„± í†µê³„ ì •ë³´ ìƒì„± ì™„ë£Œ")
        return stats_results
    
    def save_processed_data(self, df, sequences_data, numeric_features, feature_groups):
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        self.log_process("ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì‹œì‘")
        
        # 1. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ëœ ë°ì´í„° ì €ì¥
        df.to_csv(f"{self.results_base}/data/features/weekly_sentiment_features.csv", index=False)
        
        # 2. íšŒì‚¬ë³„ ë°ì´í„° ì €ì¥
        for company in df['Company'].unique():
            company_data = df[df['Company'] == company]
            company_data.to_csv(f"{self.results_base}/data/features/{company.lower()}_weekly_features.csv", index=False)
        
        # 3. LSTM ì‹œí€€ìŠ¤ ë°ì´í„° ì €ì¥
        X, y, meta_df, split_data = sequences_data
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° pickleë¡œ ì €ì¥
        lstm_data = {
            'X': X,
            'y': y,
            'meta_df': meta_df,
            'split_data': split_data,
            'feature_names': numeric_features,
            'scalers': self.scalers
        }
        
        with open(f"{self.results_base}/data/features/lstm_training_sequences.pkl", 'wb') as f:
            pickle.dump(lstm_data, f)
        
        # 4. íŠ¹ì„± ì •ë³´ ì €ì¥
        feature_info = {
            'total_features': len(numeric_features),
            'feature_names': numeric_features,
            'feature_groups': feature_groups,
            'sequence_length': 4,
            'target_column': 'sentiment_score_7d_avg',
            'companies': df['Company'].unique().tolist(),
            'date_range': {
                'start': df['Date'].min().strftime('%Y-%m-%d'),
                'end': df['Date'].max().strftime('%Y-%m-%d')
            }
        }
        
        with open(f"{self.results_base}/data/features/feature_info.json", 'w') as f:
            json.dump(feature_info, f, indent=2, ensure_ascii=False)
        
        # 5. ì²˜ë¦¬ ë¡œê·¸ ì €ì¥
        log_content = '\n'.join(self.processing_log)
        with open(f"{self.results_base}/data/processed/feature_engineering_log.txt", 'w') as f:
            f.write(log_content)
        
        self.log_process("ëª¨ë“  ë°ì´í„° ì €ì¥ ì™„ë£Œ")
    
    def generate_preprocessing_report(self, df, numeric_features, feature_groups, correlation_results, stats_results):
        """ì „ì²˜ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.log_process("ì „ì²˜ë¦¬ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
        
        report_content = f"""# 7ì¼ í‰ê·  LSTM ë°ì´í„° ì „ì²˜ë¦¬ ë¦¬í¬íŠ¸

## í”„ë¡œì íŠ¸ ì •ë³´
- **ìƒì„±ì¼**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **íŒ€**: í˜„ì¢…ë¯¼(íŒ€ì¥), ì‹ ì˜ˆì›(íŒ€ì›), ê¹€ì±„ì€(íŒ€ì›)
- **ëª©í‘œ**: ì£¼ê°„ ë°ì´í„° ê¸°ë°˜ LSTM ê°ì„± ì˜ˆì¸¡ ëª¨ë¸ ë°ì´í„° ì¤€ë¹„

## ë°ì´í„° ê°œìš”
- **ì „ì²´ ë°ì´í„°**: {len(df):,}ê±´
- **Apple ë°ì´í„°**: {len(df[df['Company'] == 'Apple']):,}ê±´
- **Samsung ë°ì´í„°**: {len(df[df['Company'] == 'Samsung']):,}ê±´
- **ê¸°ê°„**: {df['Date'].min().strftime('%Y-%m-%d')} ~ {df['Date'].max().strftime('%Y-%m-%d')}
- **ì´ íŠ¹ì„± ìˆ˜**: {len(numeric_features)}ê°œ

## íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼

### íŠ¹ì„± ê·¸ë£¹ë³„ êµ¬ì„±
"""
        
        for group, features in feature_groups.items():
            report_content += f"- **{group}**: {len(features)}ê°œ\n"
        
        report_content += f"""

### ì£¼ìš” íŠ¹ì„± ìƒê´€ê´€ê³„ (Apple vs Samsung)
- **ê°ì„±-ì£¼ê°€ ìƒê´€ê´€ê³„ (Apple)**: {correlation_results['Apple'].loc['sentiment_score_7d_avg', 'stock_price_7d_avg']:.3f}
- **ê°ì„±-ì£¼ê°€ ìƒê´€ê´€ê³„ (Samsung)**: {correlation_results['Samsung'].loc['sentiment_score_7d_avg', 'stock_price_7d_avg']:.3f}

### LSTM ì‹œí€€ìŠ¤ ì •ë³´
- **ì‹œí€€ìŠ¤ ê¸¸ì´**: 4ì£¼ (28ì¼)
- **ì˜ˆì¸¡ íƒ€ê²Ÿ**: ë‹¤ìŒ ì£¼ ê°ì„± ì ìˆ˜
- **ì •ê·œí™” ë°©ë²•**: RobustScaler (íšŒì‚¬ë³„ ê°œë³„ ì ìš©)

### ë°ì´í„° ë¶„í•  ì „ëµ
- **í›ˆë ¨ ë°ì´í„°**: 60% (ì‹œê³„ì—´ ìˆœì„œ ê¸°ì¤€)
- **ê²€ì¦ ë°ì´í„°**: 20%
- **í…ŒìŠ¤íŠ¸ ë°ì´í„°**: 20%

## í’ˆì§ˆ ê´€ë¦¬
- **ê²°ì¸¡ê°’ ì²˜ë¦¬**: ì „ì§„/í›„ì§„ ì±„ì›€ + 0ìœ¼ë¡œ ëŒ€ì²´
- **ì´ìƒê°’ ì²˜ë¦¬**: RobustScalerë¡œ ìŠ¤ì¼€ì¼ë§
- **ë¬´í•œê°’ ì²˜ë¦¬**: NaNìœ¼ë¡œ ë³€í™˜ í›„ ë³´ê°„

## íŒŒì¼ ì¶œë ¥
1. `weekly_sentiment_features.csv` - ì „ì²´ íŠ¹ì„± ë°ì´í„°
2. `apple_weekly_features.csv` - Apple íŠ¹ì„± ë°ì´í„°
3. `samsung_weekly_features.csv` - Samsung íŠ¹ì„± ë°ì´í„°
4. `lstm_training_sequences.pkl` - LSTM í›ˆë ¨ìš© ì‹œí€€ìŠ¤
5. `feature_correlation_matrix.png` - ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
6. `feature_info.json` - íŠ¹ì„± ë©”íƒ€ë°ì´í„°

## ë‹¤ìŒ ë‹¨ê³„
1. LSTM ëª¨ë¸ í›ˆë ¨ (`10.ê°œì„ ëœì‚¼ì„±LSTM.py`)
2. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (SHAP)
3. ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€ ë° ë¹„êµ

---
*ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        with open(f"{self.results_base}/reports/methodology/data_preprocessing_report.md", 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.log_process("ì „ì²˜ë¦¬ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ 7ì¼ í‰ê·  LSTM ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
    print("=" * 80)
    
    # ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = WeeklyLSTMDataProcessor()
    
    try:
        # 1. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰
        print("\nğŸ“Š Step 1: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰")
        df = processor.create_all_features()
        
        if df is None:
            print("âŒ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤íŒ¨")
            return
        
        # 2. LSTM ì‹œí€€ìŠ¤ ìƒì„±
        print("\nğŸ”„ Step 2: LSTM ì‹œí€€ìŠ¤ ìƒì„±")
        feature_columns = [col for col in df.columns if col not in ['Date', 'Company', 'Year', 'launch_category']]
        numeric_features = df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()
        
        X, y, meta_df, _ = processor.create_lstm_sequences(df)
        
        # 3. ê²€ì¦ ë°ì´í„° ë¶„í• 
        print("\nğŸ“‹ Step 3: ê²€ì¦ ë°ì´í„° ë¶„í• ")
        split_data = processor.create_validation_split(X, y, meta_df)
        
        # 4. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ì¤€ë¹„
        print("\nğŸ” Step 4: íŠ¹ì„± ë¶„ì„")
        feature_groups = processor.analyze_feature_importance_preparation(df, numeric_features)
        correlation_results = processor.create_correlation_analysis(df, numeric_features)
        stats_results = processor.create_feature_statistics(df, numeric_features)
        
        # 5. ë°ì´í„° ì €ì¥
        print("\nğŸ’¾ Step 5: ë°ì´í„° ì €ì¥")
        sequences_data = (X, y, meta_df, split_data)
        processor.save_processed_data(df, sequences_data, numeric_features, feature_groups)
        
        # 6. ë¦¬í¬íŠ¸ ìƒì„±
        print("\nğŸ“„ Step 6: ë¦¬í¬íŠ¸ ìƒì„±")
        processor.generate_preprocessing_report(df, numeric_features, feature_groups, 
                                              correlation_results, stats_results)
        
        print("\nâœ… 7ì¼ í‰ê·  LSTM ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ë¬¼ ì €ì¥ ìœ„ì¹˜: {RESULTS_BASE}")
        print("\nğŸ“ˆ ìµœì¢… ê²°ê³¼ ìš”ì•½:")
        print(f"- ì „ì²´ íŠ¹ì„± ìˆ˜: {len(numeric_features)}ê°œ")
        print(f"- LSTM ì‹œí€€ìŠ¤: {X.shape[0]:,}ê°œ (ì…ë ¥: {X.shape[1:]} â†’ ì¶œë ¥: {y.shape})")
        print(f"- í›ˆë ¨ ë°ì´í„°: {len(split_data['X_train']):,}ê°œ")
        print(f"- ê²€ì¦ ë°ì´í„°: {len(split_data['X_val']):,}ê°œ")
        print(f"- í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(split_data['X_test']):,}ê°œ")
        
        print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„: 10.ê°œì„ ëœì‚¼ì„±LSTM.py ì‹¤í–‰")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()