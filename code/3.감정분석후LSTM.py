"""
ê°ì„±ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ LSTM ë”¥ëŸ¬ë‹ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ (ì™„ì „ ìˆ˜ì • ë²„ì „)
- ì…ë ¥: samsung_sentiment_{year}.csv, apple_sentiment_{year}.csv
- ëª¨ë¸: PyTorch LSTM, GRU, Transformer ë“± ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸
- ì¶œë ¥: ë¯¸ë˜ ê°ì„± íŠ¸ë Œë“œ ì˜ˆì¸¡ ë° ì‹œê°í™”
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# PyTorch ê´€ë ¨
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ì‹œê°í™” ì„¤ì • (ì˜ì–´ í°íŠ¸ë¡œ ê°•ì œ ì„¤ì •)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
print("âœ… í°íŠ¸ ì„¤ì • ì™„ë£Œ: DejaVu Sans (í•œê¸€ ê¹¨ì§ ë°©ì§€)")

# GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

class SentimentTimeSeriesDataset(Dataset):
    """ê°ì„± ì‹œê³„ì—´ ë°ì´í„°ì…‹"""
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class AdvancedLSTMModel(nn.Module):
    """ê³ ê¸‰ LSTM ëª¨ë¸ (Attention í¬í•¨)"""
    def __init__(self, input_size, hidden_size=128, num_layers=3, dropout=0.3, use_attention=True):
        super(AdvancedLSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.use_attention = use_attention
        
        # Bidirectional LSTM
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=True,
            bidirectional=True
        )
        
        # Attention ë©”ì»¤ë‹ˆì¦˜
        if use_attention:
            self.attention = nn.MultiheadAttention(
                embed_dim=hidden_size * 2,
                num_heads=8,
                dropout=dropout,
                batch_first=True
            )
        
        # Residual connectionsì„ ìœ„í•œ layer norm
        self.layer_norm1 = nn.LayerNorm(hidden_size * 2)
        
        # ì™„ì „ì—°ê²°ì¸µë“¤
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_size * 2, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 1)
        )
        
    def forward(self, x):
        # LSTM ìˆœì „íŒŒ
        lstm_out, _ = self.lstm(x)
        
        # Attention ì ìš©
        if self.use_attention:
            attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
            # Residual connection
            lstm_out = self.layer_norm1(lstm_out + attn_out)
        
        # ë§ˆì§€ë§‰ ì‹œê°„ ìŠ¤í… ì‚¬ìš©
        out = lstm_out[:, -1, :]
        
        # ì™„ì „ì—°ê²°ì¸µ í†µê³¼
        out = self.fc_layers(out)
        
        return out

class GRUModel(nn.Module):
    """GRU ê¸°ë°˜ ëª¨ë¸"""
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.3):
        super(GRUModel, self).__init__()
        
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=True,
            bidirectional=True
        )
        
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 1)
        )
    
    def forward(self, x):
        gru_out, _ = self.gru(x)
        out = self.fc(gru_out[:, -1, :])
        return out

class TransformerModel(nn.Module):
    """Transformer ê¸°ë°˜ ì‹œê³„ì—´ ëª¨ë¸"""
    def __init__(self, input_size, d_model=128, nhead=8, num_layers=3, dropout=0.3):
        super(TransformerModel, self).__init__()
        
        self.input_projection = nn.Linear(input_size, d_model)
        self.pos_encoding = self._generate_pos_encoding(1000, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 1)
        )
    
    def _generate_pos_encoding(self, max_len, d_model):
        pos_encoding = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(np.log(10000.0) / d_model))
        
        pos_encoding[:, 0::2] = torch.sin(position * div_term)
        pos_encoding[:, 1::2] = torch.cos(position * div_term)
        
        return pos_encoding.unsqueeze(0)
    
    def forward(self, x):
        seq_len = x.size(1)
        x = self.input_projection(x)
        
        # Positional encoding ì¶”ê°€
        if seq_len <= self.pos_encoding.size(1):
            x += self.pos_encoding[:, :seq_len, :].to(x.device)
        
        # Transformer ìˆœì „íŒŒ
        transformer_out = self.transformer(x)
        
        # ë§ˆì§€ë§‰ ì‹œê°„ ìŠ¤í… ì‚¬ìš©
        out = self.fc(transformer_out[:, -1, :])
        
        return out

class SentimentDeepLearningAnalyzer:
    def __init__(self, data_dir="./data/processed"):
        """
        ê°ì„±ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ë”¥ëŸ¬ë‹ ë¶„ì„ê¸°
        
        Args:
            data_dir: ê°ì„±ë¶„ì„ ê²°ê³¼ CSV íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        """
        self.data_dir = data_dir
        self.years = ['2021', '2022', '2023', '2024']
        self.companies = ['samsung', 'apple']
        
        # ë°ì´í„° ì €ì¥ìš©
        self.raw_data = {}
        self.processed_data = {}
        self.models = {}
        self.scalers = {}
        self.results = {}
        
    def load_sentiment_data(self):
        """ì¡°ì›ì´ ìƒì„±í•œ ê°ì„±ë¶„ì„ ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë”©"""
        print("ğŸ“Š ê°ì„±ë¶„ì„ ê²°ê³¼ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        for company in self.companies:
            company_data = []
            
            for year in self.years:
                file_path = f"{self.data_dir}/{company}_sentiment_{year}.csv"
                
                try:
                    df = pd.read_csv(file_path, encoding='utf-8-sig')
                    print(f"   âœ… {file_path}: {len(df)}ê±´")
                    company_data.append(df)
                    
                except FileNotFoundError:
                    print(f"   âŒ {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                except Exception as e:
                    print(f"   âŒ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            if company_data:
                # ì—°ë„ë³„ ë°ì´í„° í†µí•©
                combined_df = pd.concat(company_data, ignore_index=True)
                self.raw_data[company] = combined_df
                print(f"ğŸ“ˆ {company.upper()} ì´ {len(combined_df)}ê±´ ë°ì´í„° ë¡œë”© ì™„ë£Œ")
            else:
                print(f"âš ï¸ {company.upper()} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return len(self.raw_data) > 0
    
    def preprocess_for_timeseries(self, company, sequence_length=30):
        """ì‹œê³„ì—´ ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬"""
        print(f"ğŸ”§ {company.upper()} ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        
        if company not in self.raw_data:
            print(f"âŒ {company} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        df = self.raw_data[company].copy()
        
        # ë‚ ì§œ ì²˜ë¦¬
        df['ì¼ì'] = pd.to_datetime(df['ì¼ì'])
        df = df.sort_values('ì¼ì').reset_index(drop=True)
        
        # ì¼ë³„ ì§‘ê³„ (ì—¬ëŸ¬ ê¸°ì‚¬ê°€ ê°™ì€ ë‚ ì— ìˆì„ ìˆ˜ ìˆìŒ)
        daily_stats = df.groupby('ì¼ì').agg({
            'ê°ì •ì ìˆ˜': ['mean', 'std', 'count'],
            'ê°ì •ë¼ë²¨': lambda x: (x == 'positive').sum() / len(x)  # ê¸ì • ë¹„ìœ¨
        }).reset_index()
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        daily_stats.columns = ['date', 'sentiment_mean', 'sentiment_std', 'news_count', 'positive_ratio']
        daily_stats['sentiment_std'] = daily_stats['sentiment_std'].fillna(0)
        
        # ì¶”ê°€ íŠ¹ì„± ìƒì„±
        daily_stats['sentiment_ma_7'] = daily_stats['sentiment_mean'].rolling(7, min_periods=1).mean()
        daily_stats['sentiment_ma_30'] = daily_stats['sentiment_mean'].rolling(30, min_periods=1).mean()
        daily_stats['news_count_ma_7'] = daily_stats['news_count'].rolling(7, min_periods=1).mean()
        
        # ê°ì„± ë³€í™”ìœ¨
        daily_stats['sentiment_change'] = daily_stats['sentiment_mean'].pct_change().fillna(0)
        daily_stats['sentiment_momentum'] = daily_stats['sentiment_change'].rolling(3, min_periods=1).mean()
        
        # ë³€ë™ì„± ì§€í‘œ
        daily_stats['sentiment_volatility'] = daily_stats['sentiment_mean'].rolling(7, min_periods=1).std().fillna(0)
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        daily_stats = daily_stats.fillna(method='ffill').fillna(method='bfill')
        
        # ì‹œê°„ íŠ¹ì„± ì¶”ê°€
        daily_stats['day_of_week'] = daily_stats['date'].dt.dayofweek
        daily_stats['month'] = daily_stats['date'].dt.month
        daily_stats['quarter'] = daily_stats['date'].dt.quarter
        
        # ì›-í•« ì¸ì½”ë”©
        daily_stats = pd.get_dummies(daily_stats, columns=['day_of_week', 'month', 'quarter'], prefix=['dow', 'mon', 'qtr'])
        
        self.processed_data[company] = daily_stats
        
        print(f"âœ… {company.upper()} ì „ì²˜ë¦¬ ì™„ë£Œ: {len(daily_stats)}ì¼ê°„ ë°ì´í„°")
        print(f"   - ê¸°ê°„: {daily_stats['date'].min()} ~ {daily_stats['date'].max()}")
        print(f"   - íŠ¹ì„± ìˆ˜: {len(daily_stats.columns) - 1}")  # date ì œì™¸
        
        return daily_stats
    
    def create_sequences(self, data, target_col, feature_cols, sequence_length=30):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        X, y = [], []
        
        for i in range(sequence_length, len(data)):
            # ì…ë ¥ ì‹œí€€ìŠ¤ (ê³¼ê±° sequence_length ì¼ê°„ì˜ íŠ¹ì„±ë“¤)
            X.append(data[feature_cols].iloc[i-sequence_length:i].values)
            # íƒ€ê²Ÿ (ë‹¤ìŒ ë‚ ì˜ ê°ì„± ì ìˆ˜)
            y.append(data[target_col].iloc[i])
        
        return np.array(X), np.array(y)
    
    def train_models(self, company, sequence_length=30, epochs=100, learning_rate=0.001):
        """ì—¬ëŸ¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨"""
        print(f"ğŸ¤– {company.upper()} ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ í›ˆë ¨ ì‹œì‘...")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        data = self.preprocess_for_timeseries(company, sequence_length)
        if data is None:
            return None
        
        # íŠ¹ì„± ì„ íƒ
        feature_cols = [col for col in data.columns if col not in ['date', 'sentiment_mean']]
        target_col = 'sentiment_mean'
        
        print(f"   ğŸ“Š ì‚¬ìš© íŠ¹ì„±: {len(feature_cols)}ê°œ")
        
        # ì •ê·œí™”
        scaler_X = MinMaxScaler()
        scaler_y = MinMaxScaler()
        
        X_scaled = scaler_X.fit_transform(data[feature_cols])
        y_scaled = scaler_y.fit_transform(data[[target_col]])
        
        # ì •ê·œí™”ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        scaled_data = pd.DataFrame(X_scaled, columns=feature_cols)
        scaled_data[target_col] = y_scaled.flatten()
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = self.create_sequences(scaled_data, target_col, feature_cols, sequence_length)
        
        if len(X) == 0:
            print("âŒ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
            return None
        
        # í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í•  (70/15/15)
        train_size = int(len(X) * 0.7)
        val_size = int(len(X) * 0.15)
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:train_size + val_size]
        y_val = y[train_size:train_size + val_size]
        X_test = X[train_size + val_size:]
        y_test = y[train_size + val_size:]
        
        print(f"   ğŸ“Š ë°ì´í„° ë¶„í• : í›ˆë ¨ {len(X_train)} / ê²€ì¦ {len(X_val)} / í…ŒìŠ¤íŠ¸ {len(X_test)}")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = SentimentTimeSeriesDataset(X_train, y_train)
        val_dataset = SentimentTimeSeriesDataset(X_val, y_val)
        test_dataset = SentimentTimeSeriesDataset(X_test, y_test)
        
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
        
        # ëª¨ë¸ë“¤ ì •ì˜
        models_to_train = {
            'LSTM': AdvancedLSTMModel(len(feature_cols), use_attention=True),
            'GRU': GRUModel(len(feature_cols)),
            'Transformer': TransformerModel(len(feature_cols)),
            'LSTM_Simple': AdvancedLSTMModel(len(feature_cols), use_attention=False)
        }
        
        company_results = {}
        
        for model_name, model in models_to_train.items():
            print(f"\n   ğŸ”¥ {model_name} ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            
            model = model.to(device)
            criterion = nn.MSELoss()
            optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)
            
            # í›ˆë ¨ ê¸°ë¡
            train_losses, val_losses = [], []
            best_val_loss = float('inf')
            patience_counter = 0
            
            for epoch in range(epochs):
                # í›ˆë ¨
                model.train()
                train_loss = 0
                for X_batch, y_batch in train_loader:
                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                    
                    optimizer.zero_grad()
                    outputs = model(X_batch)
                    loss = criterion(outputs.squeeze(), y_batch)
                    loss.backward()
                    
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    train_loss += loss.item()
                
                # ê²€ì¦
                model.eval()
                val_loss = 0
                with torch.no_grad():
                    for X_batch, y_batch in val_loader:
                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                        outputs = model(X_batch)
                        val_loss += criterion(outputs.squeeze(), y_batch).item()
                
                train_loss /= len(train_loader)
                val_loss /= len(val_loader)
                
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                
                scheduler.step(val_loss)
                
                # ì¡°ê¸° ì¢…ë£Œ
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    best_model_state = model.state_dict().copy()
                else:
                    patience_counter += 1
                    if patience_counter >= 15:
                        print(f"     â¹ï¸ ì¡°ê¸° ì¢…ë£Œ (ì—í¬í¬ {epoch+1})")
                        break
                
                if (epoch + 1) % 20 == 0:
                    print(f"     ì—í¬í¬ [{epoch+1}/{epochs}] - í›ˆë ¨: {train_loss:.4f}, ê²€ì¦: {val_loss:.4f}")
            
            # ìµœìƒì˜ ëª¨ë¸ ë¡œë“œ
            model.load_state_dict(best_model_state)
            
            # í…ŒìŠ¤íŠ¸ í‰ê°€
            model.eval()
            predictions, actuals = [], []
            
            with torch.no_grad():
                for X_batch, y_batch in test_loader:
                    X_batch = X_batch.to(device)
                    outputs = model(X_batch)
                    predictions.extend(outputs.cpu().numpy())
                    actuals.extend(y_batch.numpy())
            
            predictions = np.array(predictions).flatten()
            actuals = np.array(actuals).flatten()
            
            # ì •ê·œí™” í•´ì œ
            predictions_original = scaler_y.inverse_transform(predictions.reshape(-1, 1)).flatten()
            actuals_original = scaler_y.inverse_transform(actuals.reshape(-1, 1)).flatten()
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            mse = mean_squared_error(actuals_original, predictions_original)
            mae = mean_absolute_error(actuals_original, predictions_original)
            r2 = r2_score(actuals_original, predictions_original)
            
            print(f"     âœ… {model_name} ì™„ë£Œ - RÂ²: {r2:.4f}, MAE: {mae:.4f}")
            
            # ê²°ê³¼ ì €ì¥
            company_results[model_name] = {
                'model': model,
                'train_losses': train_losses,
                'val_losses': val_losses,
                'predictions': predictions_original,
                'actuals': actuals_original,
                'mse': mse,
                'mae': mae,
                'r2': r2
            }
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        self.models[company] = company_results
        self.scalers[company] = {'scaler_X': scaler_X, 'scaler_y': scaler_y}
        self.results[company] = {
            'data': data,
            'feature_cols': feature_cols,
            'target_col': target_col,
            'sequence_length': sequence_length,
            'test_dates': data['date'].iloc[train_size + val_size + sequence_length:]
        }
        
        return company_results
    
    def compare_models(self, company):
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        if company not in self.models:
            print(f"âŒ {company} ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ† {company.upper()} ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        print("-" * 60)
        
        results_df = []
        for model_name, result in self.models[company].items():
            results_df.append({
                'Model': model_name,
                'RÂ²': result['r2'],
                'MAE': result['mae'],
                'MSE': result['mse']
            })
        
        df = pd.DataFrame(results_df).sort_values('RÂ²', ascending=False)
        print(df.to_string(index=False, float_format='%.4f'))
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = df.iloc[0]['Model']
        print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model} (RÂ² = {df.iloc[0]['RÂ²']:.4f})")
        
        return df
    
    """
ì‹œê°í™” ì œëª© ê²¹ì¹¨ ë¬¸ì œ ìˆ˜ì • ì½”ë“œ
visualize_results ë©”ì„œë“œì˜ ìˆ˜ì •ëœ ë¶€ë¶„
"""

def visualize_results(self, company):
    """ê²°ê³¼ ì‹œê°í™” (ì œëª© ê²¹ì¹¨ í•´ê²°)"""
    if company not in self.models:
        print(f"âŒ {company} ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“Š {company.upper()} ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    
    # ì˜ì–´ í°íŠ¸ ê°•ì œ ì„¤ì •
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(3, 3, figsize=(20, 18))  # ë†’ì´ë¥¼ 18ë¡œ ì¦ê°€
    fig.suptitle(f'{company.upper()} Deep Learning Model Analysis Results', 
                fontsize=16, fontweight='bold', y=0.98)  # y ìœ„ì¹˜ ì¡°ì •
    
    # 1. ì›ë³¸ ë°ì´í„° íŠ¸ë Œë“œ
    ax1 = axes[0, 0]
    data = self.results[company]['data']
    ax1.plot(data['date'], data['sentiment_mean'], alpha=0.7, color='blue', label='Daily')
    ax1.plot(data['date'], data['sentiment_ma_30'], color='red', linewidth=2, label='30-day MA')
    ax1.set_title('Sentiment Score Trend', fontsize=12, fontweight='bold', pad=15)
    ax1.set_ylabel('Sentiment Score', fontsize=10)
    ax1.legend(fontsize=9)
    ax1.grid(True, alpha=0.3)
    ax1.tick_params(axis='x', rotation=45, labelsize=8)
    ax1.tick_params(axis='y', labelsize=8)
    
    # 2. ëª¨ë¸ë³„ í›ˆë ¨ ê³¼ì •
    ax2 = axes[0, 1]
    colors = ['blue', 'red', 'green', 'orange']
    for i, (model_name, result) in enumerate(self.models[company].items()):
        color = colors[i % len(colors)]
        epochs_to_show = min(50, len(result['train_losses']))
        ax2.plot(result['train_losses'][:epochs_to_show], 
                label=f'{model_name} Train', alpha=0.7, color=color, linewidth=1.5)
        ax2.plot(result['val_losses'][:epochs_to_show], 
                label=f'{model_name} Val', linestyle='--', alpha=0.7, color=color, linewidth=1.5)
    ax2.set_title(f'Training Process (First {epochs_to_show} Epochs)', 
                 fontsize=12, fontweight='bold', pad=15)
    ax2.set_ylabel('Loss', fontsize=10)
    ax2.set_xlabel('Epoch', fontsize=10)
    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    ax2.grid(True, alpha=0.3)
    ax2.tick_params(labelsize=8)
    
    # 3. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    ax3 = axes[0, 2]
    model_names = list(self.models[company].keys())
    r2_scores = [self.models[company][name]['r2'] for name in model_names]
    
    # ìƒ‰ìƒ êµ¬ë¶„ (ì–‘ìˆ˜ëŠ” íŒŒë€ìƒ‰, ìŒìˆ˜ëŠ” ë¹¨ê°„ìƒ‰)
    colors = ['skyblue' if score >= 0 else 'lightcoral' for score in r2_scores]
    bars = ax3.bar(model_names, r2_scores, color=colors)
    
    ax3.set_title('Model Performance Comparison (RÂ²)', fontsize=12, fontweight='bold', pad=15)
    ax3.set_ylabel('RÂ² Score', fontsize=10)
    ax3.tick_params(axis='x', rotation=45, labelsize=8)
    ax3.tick_params(axis='y', labelsize=8)
    ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)  # 0ì„  ì¶”ê°€
    
    # ê°’ í‘œì‹œ
    for bar, score in zip(bars, r2_scores):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., 
                height + (0.01 if height >= 0 else -0.02),
                f'{score:.3f}', ha='center', 
                va='bottom' if height >= 0 else 'top', fontsize=9)
    
    # 4-6. ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ vs ì‹¤ì œ (ìƒìœ„ 3ê°œ ëª¨ë¸)
    sorted_models = sorted(self.models[company].items(), 
                         key=lambda x: x[1]['r2'], reverse=True)
    
    test_dates = self.results[company]['test_dates'].reset_index(drop=True)
    
    for i, (model_name, result) in enumerate(sorted_models[:3]):
        ax = axes[1, i]
        ax.plot(test_dates, result['actuals'], 
               label='Actual', alpha=0.8, linewidth=2, color='blue')
        ax.plot(test_dates, result['predictions'], 
               label='Predicted', alpha=0.8, linewidth=2, linestyle='--', color='red')
        ax.set_title(f'{model_name} Prediction Results\n(RÂ²={result["r2"]:.3f})', 
                    fontsize=11, fontweight='bold', pad=15)
        ax.set_ylabel('Sentiment Score', fontsize=10)
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.tick_params(axis='x', rotation=45, labelsize=8)
        ax.tick_params(axis='y', labelsize=8)
    
    # 7-9. ì‚°ì ë„ (ì˜ˆì¸¡ vs ì‹¤ì œ)
    for i, (model_name, result) in enumerate(sorted_models[:3]):
        ax = axes[2, i]
        ax.scatter(result['actuals'], result['predictions'], 
                  alpha=0.6, s=20, color='blue')
        
        # ì™„ë²½í•œ ì˜ˆì¸¡ì„ 
        min_val = min(result['actuals'].min(), result['predictions'].min())
        max_val = max(result['actuals'].max(), result['predictions'].max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)
        
        ax.set_xlabel('Actual Values', fontsize=10)
        ax.set_ylabel('Predicted Values', fontsize=10)
        ax.set_title(f'{model_name} Scatter Plot\n(RÂ²={result["r2"]:.3f})', 
                    fontsize=11, fontweight='bold', pad=15)
        ax.grid(True, alpha=0.3)
        ax.tick_params(labelsize=8)
    
    # ë ˆì´ì•„ì›ƒ ì¡°ì •
    plt.tight_layout(rect=[0, 0.03, 1, 0.96])  # ì—¬ë°± ì¡°ì •
    plt.subplots_adjust(hspace=0.4, wspace=0.3)  # subplot ê°„ê²© ì¦ê°€
    
    plt.savefig(f'{company}_deeplearning_results_fixed.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.show()
    
    print(f"âœ… {company.upper()} ì‹œê°í™” ì™„ë£Œ (íŒŒì¼: {company}_deeplearning_results_fixed.png)")
    
    def predict_future(self, company, days=30, model_name='best'):
        """ë¯¸ë˜ ì˜ˆì¸¡"""
        if company not in self.models:
            print(f"âŒ {company} ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        if model_name == 'best':
            best_model_name = max(self.models[company].items(), key=lambda x: x[1]['r2'])[0]
            model = self.models[company][best_model_name]['model']
            print(f"ğŸ¯ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©: {best_model_name}")
        else:
            if model_name not in self.models[company]:
                print(f"âŒ {model_name} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            model = self.models[company][model_name]['model']
        
        print(f"ğŸ”® {company.upper()} í–¥í›„ {days}ì¼ ì˜ˆì¸¡ ì¤‘...")
        
        # ë°ì´í„° ë° ìŠ¤ì¼€ì¼ëŸ¬ ê°€ì ¸ì˜¤ê¸°
        data = self.results[company]['data']
        feature_cols = self.results[company]['feature_cols']
        sequence_length = self.results[company]['sequence_length']
        scaler_X = self.scalers[company]['scaler_X']
        scaler_y = self.scalers[company]['scaler_y']
        
        # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì¤€ë¹„
        last_sequence = scaler_X.transform(data[feature_cols].tail(sequence_length))
        
        model.eval()
        predictions = []
        current_sequence = torch.FloatTensor(last_sequence).unsqueeze(0).to(device)
        
        with torch.no_grad():
            for day in range(days):
                # ì˜ˆì¸¡ ìˆ˜í–‰
                pred_scaled = model(current_sequence)
                pred_value = pred_scaled.cpu().numpy()[0, 0]
                
                # ì •ê·œí™” í•´ì œ
                pred_original = scaler_y.inverse_transform([[pred_value]])[0, 0]
                predictions.append(pred_original)
                
                # ë‹¤ìŒ ì‹œí€€ìŠ¤ë¥¼ ìœ„í•œ íŠ¹ì„± ì—…ë°ì´íŠ¸ (ê°„ë‹¨í•œ ë°©ë²•)
                new_features = current_sequence[0, -1].clone()
                new_features[0] = pred_scaled[0, 0]  # ì²« ë²ˆì§¸ íŠ¹ì„±(ê°ì„±ì ìˆ˜)ì„ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                
                # ì‹œí€€ìŠ¤ ë¡¤ë§
                current_sequence = torch.cat([
                    current_sequence[:, 1:, :],
                    new_features.unsqueeze(0).unsqueeze(0)
                ], dim=1)
        
        # ë¯¸ë˜ ë‚ ì§œ ìƒì„±
        last_date = data['date'].max()
        future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=days)
        
        future_df = pd.DataFrame({
            'date': future_dates,
            'predicted_sentiment': predictions
        })
        
        print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ")
        print(f"   - í‰ê·  ì˜ˆì¸¡ ê°ì„±: {np.mean(predictions):.3f}")
        print(f"   - ì˜ˆì¸¡ ë²”ìœ„: {np.min(predictions):.3f} ~ {np.max(predictions):.3f}")
        
        return future_df
    
    def visualize_future_prediction(self, company, days=30):
        """ë¯¸ë˜ ì˜ˆì¸¡ ì‹œê°í™” (ì˜ì–´ ì œëª©)"""
        future_pred = self.predict_future(company, days)
        if future_pred is None:
            return
        
        # ì˜ì–´ í°íŠ¸ ê°•ì œ ì„¤ì •
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        
        data = self.results[company]['data']
        
        plt.figure(figsize=(16, 10))
        
        # ìƒë‹¨: ì „ì²´ íŠ¸ë Œë“œ + ë¯¸ë˜ ì˜ˆì¸¡
        plt.subplot(2, 1, 1)
        
        # ê³¼ê±° ë°ì´í„° (ìµœê·¼ 120ì¼)
        recent_data = data.tail(120) if len(data) > 120 else data
        plt.plot(recent_data['date'], recent_data['sentiment_mean'], 
                'b-', label='Past Actual Sentiment', linewidth=2, alpha=0.8)
        plt.plot(recent_data['date'], recent_data['sentiment_ma_30'], 
                'orange', label='30-day Moving Average', linewidth=2, alpha=0.8)
        
        # ë¯¸ë˜ ì˜ˆì¸¡
        plt.plot(future_pred['date'], future_pred['predicted_sentiment'], 
                'r--', label='Future Prediction', linewidth=3, marker='o', markersize=4)
        
        # ì‹ ë¢°êµ¬ê°„
        plt.fill_between(future_pred['date'], 
                        future_pred['predicted_sentiment'] - 0.15,
                        future_pred['predicted_sentiment'] + 0.15,
                        alpha=0.2, color='red', label='Prediction Confidence Interval')
        
        # í˜„ì¬ ì‹œì  í‘œì‹œ
        plt.axvline(x=data['date'].max(), color='green', linestyle='-', 
                   alpha=0.8, linewidth=2, label='Current Time')
        
        plt.title(f'{company.upper()} Sentiment Score Future Prediction ({days} days)', fontsize=16, fontweight='bold')
        plt.xlabel('Date')
        plt.ylabel('Sentiment Score')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        
        # í•˜ë‹¨: ì˜ˆì¸¡ íŠ¸ë Œë“œ ë¶„ì„
        plt.subplot(2, 1, 2)
        
        # ì˜ˆì¸¡ê°’ì˜ ì¼ë³„ ë³€í™”
        pred_changes = np.diff(future_pred['predicted_sentiment'].values)
        colors = ['red' if x < 0 else 'green' for x in pred_changes]
        
        plt.bar(range(len(pred_changes)), pred_changes, color=colors, alpha=0.7)
        plt.title(f'Daily Sentiment Change Prediction (Next {days} days)', fontsize=14, fontweight='bold')
        plt.xlabel('Prediction Day')
        plt.ylabel('Sentiment Score Change')
        plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        plt.grid(True, alpha=0.3)
        
        # íŠ¸ë Œë“œ ë°©í–¥ í‘œì‹œ
        overall_trend = future_pred['predicted_sentiment'].iloc[-1] - future_pred['predicted_sentiment'].iloc[0]
        trend_text = "Upward Trend" if overall_trend > 0 else "Downward Trend" if overall_trend < 0 else "Sideways Trend"
        plt.text(0.02, 0.95, f'Overall Trend: {trend_text} ({overall_trend:+.3f})', 
                transform=plt.gca().transAxes, fontsize=12, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
        
        plt.tight_layout()
        plt.savefig(f'{company}_future_prediction_final.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… ë¯¸ë˜ ì˜ˆì¸¡ ì‹œê°í™” ì™„ë£Œ (íŒŒì¼: {company}_future_prediction_final.png)")
    
    def generate_model_comparison_report(self):
        """ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ğŸ“ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        report = []
        report.append("=" * 80)
        report.append("ğŸ¤– ê°ì„±ë¶„ì„ ê¸°ë°˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸")
        report.append("=" * 80)
        report.append(f"ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
        report.append("")
        
        # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
        report.append("ğŸ“Š ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½")
        report.append("-" * 50)
        
        all_results = []
        for company in self.models.keys():
            for model_name, result in self.models[company].items():
                all_results.append({
                    'Company': company.upper(),
                    'Model': model_name,
                    'RÂ²': result['r2'],
                    'MAE': result['mae'],
                    'MSE': result['mse']
                })
        
        if all_results:
            results_df = pd.DataFrame(all_results)
            
            # ê¸°ì—…ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸
            for company in self.models.keys():
                company_results = results_df[results_df['Company'] == company.upper()]
                if len(company_results) > 0:
                    best_model = company_results.loc[company_results['RÂ²'].idxmax()]
                    
                    report.append(f"{company.upper()} ìµœê³  ì„±ëŠ¥:")
                    report.append(f"  - ëª¨ë¸: {best_model['Model']}")
                    report.append(f"  - RÂ²: {best_model['RÂ²']:.4f}")
                    report.append(f"  - MAE: {best_model['MAE']:.4f}")
                    report.append("")
            
            # ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥
            report.append("ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥:")
            model_avg = results_df.groupby('Model')[['RÂ²', 'MAE', 'MSE']].mean()
            for model_name, row in model_avg.iterrows():
                report.append(f"  {model_name}: RÂ²={row['RÂ²']:.4f}, MAE={row['MAE']:.4f}")
            report.append("")
        
        # ë°ì´í„° íŠ¹ì„± ë¶„ì„
        report.append("ğŸ“ˆ ë°ì´í„° íŠ¹ì„± ë¶„ì„")
        report.append("-" * 50)
        
        for company in self.results.keys():
            data = self.results[company]['data']
            
            report.append(f"{company.upper()} ë°ì´í„°:")
            report.append(f"  - ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(data)}ì¼")
            report.append(f"  - ê¸°ê°„: {data['date'].min()} ~ {data['date'].max()}")
            report.append(f"  - í‰ê·  ê°ì„± ì ìˆ˜: {data['sentiment_mean'].mean():.3f}")
            report.append(f"  - ê°ì„± ì ìˆ˜ í‘œì¤€í¸ì°¨: {data['sentiment_mean'].std():.3f}")
            report.append(f"  - ì¼í‰ê·  ë‰´ìŠ¤ ìˆ˜: {data['news_count'].mean():.1f}ê±´")
            report.append("")
        
        # ë¯¸ë˜ ì˜ˆì¸¡ ë¶„ì„
        report.append("ğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ë¶„ì„")
        report.append("-" * 50)
        
        for company in self.models.keys():
            future_pred = self.predict_future(company, days=14, model_name='best')
            if future_pred is not None:
                current_sentiment = self.results[company]['data']['sentiment_mean'].iloc[-1]
                future_avg = future_pred['predicted_sentiment'].mean()
                trend_change = future_avg - current_sentiment
                
                report.append(f"{company.upper()} 14ì¼ ì˜ˆì¸¡:")
                report.append(f"  - í˜„ì¬ ê°ì„±: {current_sentiment:.3f}")
                report.append(f"  - ì˜ˆì¸¡ í‰ê· : {future_avg:.3f}")
                report.append(f"  - ì˜ˆìƒ ë³€í™”: {'+' if trend_change > 0 else ''}{trend_change:.3f}")
                
                volatility = future_pred['predicted_sentiment'].std()
                report.append(f"  - ì˜ˆì¸¡ ë³€ë™ì„±: {volatility:.3f}")
                
                if abs(trend_change) > 0.1:
                    direction = "ê°œì„ " if trend_change > 0 else "ì•…í™”"
                    report.append(f"  - íŠ¸ë Œë“œ ì „ë§: {direction} ì˜ˆìƒ")
                else:
                    report.append(f"  - íŠ¸ë Œë“œ ì „ë§: ì•ˆì •ì  ìœ ì§€")
                report.append("")
        
        # ì¸ì‚¬ì´íŠ¸ ë° ê¶Œê³ ì‚¬í•­
        report.append("ğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ë° ê¶Œê³ ì‚¬í•­")
        report.append("-" * 50)
        
        insights = []
        
        # ëª¨ë¸ ì„±ëŠ¥ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        if all_results:
            best_overall = max(all_results, key=lambda x: x['RÂ²'])
            insights.append(f"ì „ì²´ ìµœê³  ì„±ëŠ¥: {best_overall['Company']} {best_overall['Model']} (RÂ²={best_overall['RÂ²']:.4f})")
        
        for i, insight in enumerate(insights, 1):
            report.append(f"{i}. {insight}")
        
        report.append("")
        report.append("ğŸ“‹ í™œìš© ê¶Œê³ ì‚¬í•­")
        report.append("-" * 50)
        
        recommendations = [
            "ì‹¤ì‹œê°„ ê°ì„± ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶• (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í™œìš©)",
            "ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±ìœ¼ë¡œ ì˜ˆì¸¡ ì•ˆì •ì„± í–¥ìƒ",
            "ì™¸ë¶€ ë³€ìˆ˜(ì£¼ê°€, ê²½ì œì§€í‘œ) ì¶”ê°€ë¡œ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ",
            "A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ ì§€ì†ì  ê²€ì¦",
            "ê°ì„± ê¸‰ë³€ ì‹œì  ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•",
            "ë¶„ê¸°ë³„ ëª¨ë¸ ì¬í›ˆë ¨ìœ¼ë¡œ ìµœì‹  íŠ¸ë Œë“œ ë°˜ì˜"
        ]
        
        for i, rec in enumerate(recommendations, 1):
            report.append(f"{i}. {rec}")
        
        report.append("")
        report.append("=" * 80)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_text = "\n".join(report)
        with open("DeepLearning_Model_Comparison_Report.txt", "w", encoding="utf-8") as f:
            f.write(report_text)
        
        print("âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
        return report_text
    
    def run_complete_analysis(self):
        """ì „ì²´ ë”¥ëŸ¬ë‹ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ ê°ì„±ë¶„ì„ ê¸°ë°˜ ë”¥ëŸ¬ë‹ ë¶„ì„ ì‹œì‘!")
        print("=" * 80)
        
        start_time = datetime.now()
        
        try:
            # 1. ê°ì„±ë¶„ì„ ê²°ê³¼ ë¡œë”©
            if not self.load_sentiment_data():
                print("âŒ ê°ì„±ë¶„ì„ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
                return False
            
            # 2. ê° ê¸°ì—…ë³„ ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨
            trained_companies = []
            
            for company in self.companies:
                if company in self.raw_data:
                    print(f"\n2ï¸âƒ£ {company.upper()} ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨")
                    results = self.train_models(company)
                    if results:
                        trained_companies.append(company)
                        
                        # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
                        self.compare_models(company)
                else:
                    print(f"âš ï¸ {company.upper()} ë°ì´í„° ì—†ìŒ")
            
            if not trained_companies:
                print("âŒ í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 3. ê²°ê³¼ ì‹œê°í™”
            print("\n3ï¸âƒ£ ê²°ê³¼ ì‹œê°í™”")
            for company in trained_companies:
                self.visualize_results(company)
                self.visualize_future_prediction(company, days=30)
            
            # 4. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
            print("\n4ï¸âƒ£ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±")
            self.generate_model_comparison_report()
            
            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            end_time = datetime.now()
            execution_time = end_time - start_time
            
            print("\nğŸ‰ ë”¥ëŸ¬ë‹ ë¶„ì„ ì™„ë£Œ!")
            print("=" * 80)
            print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {execution_time}")
            print(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ ê¸°ì—…: {', '.join([c.upper() for c in trained_companies])}")
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìš”ì•½
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸:")
            for company in trained_companies:
                best_model = max(self.models[company].items(), key=lambda x: x[1]['r2'])
                print(f"   {company.upper()}: {best_model[0]} (RÂ²={best_model[1]['r2']:.4f})")
            
            print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
            for company in trained_companies:
                print(f"   - {company}_deeplearning_results_final.png")
                print(f"   - {company}_future_prediction_final.png")
            print(f"   - DeepLearning_Model_Comparison_Report.txt")
            
            return True
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return False


# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    try:
        print("ğŸ”— ì¡°ì›ì˜ ê°ì„±ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ë”¥ëŸ¬ë‹ ë¶„ì„ ì‹œì‘")
        print("ğŸ“ í•„ìš” íŒŒì¼: samsung_sentiment_2021.csv ~ apple_sentiment_2024.csv")
        
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = SentimentDeepLearningAnalyzer("./data/processed")
        
        # ì „ì²´ ë¶„ì„ ì‹¤í–‰
        success = analyzer.run_complete_analysis()
        
        if success:
            print("\n" + "="*60)
            print("ğŸŠ ë”¥ëŸ¬ë‹ ë¶„ì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
            print("="*60)
            
            # ëŒ€í™”í˜• ì¶”ê°€ ë¶„ì„ ì œì•ˆ
            while True:
                print("\nğŸ” ì¶”ê°€ ë¶„ì„ ì˜µì…˜:")
                print("1. íŠ¹ì • ê¸°ì—… ìƒì„¸ ë¶„ì„")
                print("2. ì»¤ìŠ¤í…€ ë¯¸ë˜ ì˜ˆì¸¡")
                print("3. ëª¨ë¸ ì„±ëŠ¥ ì¬ë¹„êµ")
                print("4. ì¢…ë£Œ")
                
                choice = input("ì„ íƒí•˜ì„¸ìš” (1-4): ").strip()
                
                if choice == '1':
                    company = input("ê¸°ì—…ëª… (samsung/apple): ").strip().lower()
                    if company in analyzer.models:
                        analyzer.visualize_results(company)
                        analyzer.compare_models(company)
                    else:
                        print(f"âŒ {company} ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                elif choice == '2':
                    company = input("ê¸°ì—…ëª… (samsung/apple): ").strip().lower()
                    try:
                        days = int(input("ì˜ˆì¸¡ ê¸°ê°„ (ì¼): ").strip())
                        if company in analyzer.models and days > 0:
                            analyzer.visualize_future_prediction(company, days)
                        else:
                            print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ì…ë‹ˆë‹¤.")
                    except ValueError:
                        print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                
                elif choice == '3':
                    for company in analyzer.models.keys():
                        analyzer.compare_models(company)
                
                elif choice == '4':
                    print("ğŸ‘‹ ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                else:
                    print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì„ íƒì…ë‹ˆë‹¤.")
        
        else:
            print("\nâŒ ë”¥ëŸ¬ë‹ ë¶„ì„ ì‹¤íŒ¨")
            print("í™•ì¸ì‚¬í•­:")
            print("1. ./data/processed/ í´ë”ì— CSV íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸")
            print("2. CSV íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
            print("3. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì •")
    
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


"""
ğŸ“‹ ì½”ë“œ ì‚¬ìš©ë²•:

1. ì¡°ì›ì˜ ê°ì„±ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì¤€ë¹„:
   - ./data/processed/samsung_sentiment_2021.csv
   - ./data/processed/samsung_sentiment_2022.csv
   - ./data/processed/samsung_sentiment_2023.csv  
   - ./data/processed/samsung_sentiment_2024.csv
   - ./data/processed/apple_sentiment_2021.csv
   - ./data/processed/apple_sentiment_2022.csv
   - ./data/processed/apple_sentiment_2023.csv
   - ./data/processed/apple_sentiment_2024.csv

2. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:
   pip install torch pandas numpy matplotlib seaborn scikit-learn tqdm

3. ì‹¤í–‰:
   python final_deeplearning_analysis.py

ğŸ¯ ì£¼ìš” íŠ¹ì§•:
âœ… 4ê°€ì§€ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¹„êµ (LSTM, GRU, Transformer, Simple LSTM)
âœ… Attention ë©”ì»¤ë‹ˆì¦˜ ì ìš© ê³ ê¸‰ LSTM
âœ… ìë™í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
âœ… ì˜ì–´ ì œëª©ìœ¼ë¡œ í•œê¸€ ê¹¨ì§ ì™„ì „ í•´ê²°
âœ… ì¢…í•©ì ì¸ ì„±ëŠ¥ ë¹„êµ ë° ì‹œê°í™”
âœ… ë¯¸ë˜ ê°ì„± íŠ¸ë Œë“œ ì˜ˆì¸¡
âœ… ìƒì„¸í•œ ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸
âœ… ëŒ€í™”í˜• ì¶”ê°€ ë¶„ì„ ê¸°ëŠ¥

ğŸš€ ì™„ì „íˆ ì‘ë™í•˜ëŠ” ìµœì¢… ë²„ì „ì…ë‹ˆë‹¤!
"""