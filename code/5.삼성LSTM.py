import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Deep Learning Libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import TimeSeriesSplit
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

print("ğŸš€ ì‚¼ì„±ì „ì ê°ì„± ë¶„ì„ ê³ ë„í™” LSTM ëª¨ë¸")
print("=" * 60)

class SamsungDataLoader:
    """ì‚¼ì„± ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, base_path):
        self.base_path = base_path
        self.data = None
        
    def load_all_years(self):
        """2021-2024ë…„ ëª¨ë“  ë°ì´í„° ë¡œë”© (UTF-8 ì¸ì½”ë”© ì‚¬ìš©)"""
        all_data = []
        
        for year in range(2021, 2025):
            try:
                file_path = f"{self.base_path}/samsung_sentiment_{year}.csv"
                df = pd.read_csv(file_path, encoding='utf-8')  # UTF-8ë¡œ ê³ ì •
                df['year'] = year
                all_data.append(df)
                print(f"âœ… {year}ë…„ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df)}ê°œ ê¸°ì‚¬")
            except FileNotFoundError:
                print(f"âŒ {year}ë…„ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            except Exception as e:
                print(f"âŒ {year}ë…„ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
                
        if all_data:
            self.data = pd.concat(all_data, ignore_index=True)
            print(f"\nğŸ“Š ì „ì²´ ë°ì´í„° í†µí•© ì™„ë£Œ: {len(self.data)}ê°œ ê¸°ì‚¬")
            print(f"ğŸ“‹ ì²« 5í–‰ ë°ì´í„° í™•ì¸:")
            print(self.data.head())
            return self.data
        else:
            raise Exception("âŒ ë¡œë”©ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    def preprocess_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"""
        if self.data is None:
            raise Exception("ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë”©í•´ì£¼ì„¸ìš”.")
        
        # ë°ì´í„° ì»¬ëŸ¼ í™•ì¸
        print(f"ğŸ“‹ ë°ì´í„° ì»¬ëŸ¼ ëª©ë¡: {list(self.data.columns)}")
        print(f"ğŸ“Š ë°ì´í„° í˜•íƒœ: {self.data.shape}")
        
        # ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ì¶˜ ì»¬ëŸ¼ ë§¤í•‘
        date_col = 'ì¼ì'
        sentiment_col = 'ê°ì •ì ìˆ˜'
        title_col = 'ì œëª©'
        keyword_col = 'í‚¤ì›Œë“œ'
        label_col = 'ê°ì •ë¼ë²¨'
        
        print(f"âœ… ë‚ ì§œ ì»¬ëŸ¼: '{date_col}'")
        print(f"âœ… ê°ì„± ì ìˆ˜ ì»¬ëŸ¼: '{sentiment_col}'")
        print(f"âœ… ì œëª© ì»¬ëŸ¼: '{title_col}'")
        print(f"âœ… í‚¤ì›Œë“œ ì»¬ëŸ¼: '{keyword_col}'")
        print(f"âœ… ê°ì •ë¼ë²¨ ì»¬ëŸ¼: '{label_col}'")
            
        # ë‚ ì§œ íŒŒì‹±
        self.data['date'] = pd.to_datetime(self.data[date_col], errors='coerce')
        self.data = self.data.dropna(subset=['date'])
        
        # ê°ì„± ì ìˆ˜ ì²˜ë¦¬
        self.data['sentiment_score'] = pd.to_numeric(self.data[sentiment_col], errors='coerce')
        self.data = self.data.dropna(subset=['sentiment_score'])
        
        # ê°ì •ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜ (ì¶”ê°€ íŠ¹ì„±ìœ¼ë¡œ í™œìš©)
        label_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}
        self.data['sentiment_label_numeric'] = self.data[label_col].map(label_mapping)
        
        # ì œëª© ê¸¸ì´ íŠ¹ì„± ì¶”ê°€
        self.data['title_length'] = self.data[title_col].str.len()
        
        # í‚¤ì›Œë“œ ê°œìˆ˜ íŠ¹ì„± ì¶”ê°€
        self.data['keyword_count'] = self.data[keyword_col].str.count(',') + 1
            
        self.data['sentiment_score'] = pd.to_numeric(self.data[sentiment_col], errors='coerce')
        self.data = self.data.dropna(subset=['sentiment_score'])
        
        # ê°ì •ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜ (ì¶”ê°€ íŠ¹ì„±ìœ¼ë¡œ í™œìš©)
        label_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}
        self.data['sentiment_label_numeric'] = self.data[label_col].map(label_mapping)
        
        # ì œëª© ê¸¸ì´ íŠ¹ì„± ì¶”ê°€
        self.data['title_length'] = self.data[title_col].str.len()
        
        # í‚¤ì›Œë“œ ê°œìˆ˜ íŠ¹ì„± ì¶”ê°€
        self.data['keyword_count'] = self.data[keyword_col].str.count(',') + 1
        
        # ì¼ë³„ ì§‘ê³„ ë° íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
        daily_data = self.data.groupby('date').agg({
            'sentiment_score': ['mean', 'std', 'count', 'min', 'max'],
            'sentiment_label_numeric': ['mean', 'sum'],  # ê¸ì •/ë¶€ì • ë¹„ìœ¨
            'title_length': 'mean',
            'keyword_count': 'mean'
        }).round(4)
        
        daily_data.columns = [
            'sentiment_mean', 'sentiment_std', 'news_count', 'sentiment_min', 'sentiment_max',
            'label_ratio', 'positive_news_count', 'avg_title_length', 'avg_keyword_count'
        ]
        daily_data = daily_data.reset_index()
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        daily_data['sentiment_std'] = daily_data['sentiment_std'].fillna(0)
        
        # ì¶”ê°€ íŠ¹ì„± ìƒì„±
        daily_data['sentiment_range'] = daily_data['sentiment_max'] - daily_data['sentiment_min']
        daily_data['sentiment_momentum_3'] = daily_data['sentiment_mean'].rolling(3).mean()
        daily_data['sentiment_momentum_7'] = daily_data['sentiment_mean'].rolling(7).mean()
        daily_data['volatility_7'] = daily_data['sentiment_mean'].rolling(7).std()
        daily_data['news_volume_ma_7'] = daily_data['news_count'].rolling(7).mean()
        
        # ìƒˆë¡œìš´ íŠ¹ì„±ë“¤
        daily_data['positive_ratio'] = daily_data['positive_news_count'] / daily_data['news_count']
        daily_data['sentiment_velocity'] = daily_data['sentiment_mean'].diff()  # ê°ì„± ë³€í™”ìœ¨
        daily_data['news_surge'] = (daily_data['news_count'] > daily_data['news_volume_ma_7'] * 1.5).astype(int)
        
        # ìš”ì¼ ë° ì›” íŠ¹ì„±
        daily_data['weekday'] = daily_data['date'].dt.dayofweek
        daily_data['month'] = daily_data['date'].dt.month
        daily_data['is_weekend'] = (daily_data['weekday'] >= 5).astype(int)
        
        # ì •ë ¬ ë° ê²°ì¸¡ê°’ ì œê±°
        daily_data = daily_data.sort_values('date')
        daily_data = daily_data.dropna()
        
        print(f"ğŸ“ˆ ì „ì²˜ë¦¬ ì™„ë£Œ: {len(daily_data)}ì¼ ë°ì´í„°")
        print(f"ğŸ“… ê¸°ê°„: {daily_data['date'].min()} ~ {daily_data['date'].max()}")
        print(f"ğŸ“Š ê°ì„± ì ìˆ˜ ë²”ìœ„: {daily_data['sentiment_mean'].min():.3f} ~ {daily_data['sentiment_mean'].max():.3f}")
        print(f"ğŸ“° ì¼í‰ê·  ë‰´ìŠ¤ ìˆ˜: {daily_data['news_count'].mean():.1f}ê°œ")
        
        return daily_data

class AdvancedLSTMDataset(Dataset):
    """ê³ ë„í™”ëœ LSTM ë°ì´í„°ì…‹"""
    
    def __init__(self, data, sequence_length=30, prediction_length=1):
        self.data = data
        self.sequence_length = sequence_length
        self.prediction_length = prediction_length
        
        # íŠ¹ì„± ì„ íƒ (ìƒˆë¡œìš´ íŠ¹ì„±ë“¤ í¬í•¨)
        self.feature_columns = [
            'sentiment_mean', 'sentiment_std', 'news_count', 'sentiment_range',
            'sentiment_momentum_3', 'sentiment_momentum_7', 'volatility_7',
            'news_volume_ma_7', 'positive_ratio', 'sentiment_velocity', 'news_surge',
            'avg_title_length', 'avg_keyword_count', 'weekday', 'month', 'is_weekend'
        ]
        
        self.target_column = 'sentiment_mean'
        
        # ìŠ¤ì¼€ì¼ë§
        self.scaler_features = RobustScaler()
        self.scaler_target = RobustScaler()
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        self.features = self.scaler_features.fit_transform(data[self.feature_columns])
        self.targets = self.scaler_target.fit_transform(data[[self.target_column]])
        
        self.sequences, self.labels = self._create_sequences()
        
    def _create_sequences(self):
        sequences = []
        labels = []
        
        for i in range(len(self.features) - self.sequence_length - self.prediction_length + 1):
            seq = self.features[i:i + self.sequence_length]
            label = self.targets[i + self.sequence_length:i + self.sequence_length + self.prediction_length]
            sequences.append(seq)
            labels.append(label)
            
        return np.array(sequences), np.array(labels)
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return torch.FloatTensor(self.sequences[idx]), torch.FloatTensor(self.labels[idx])

class AdvancedLSTMModel(nn.Module):
    """ê³ ë„í™”ëœ LSTM ëª¨ë¸"""
    
    def __init__(self, input_size, hidden_size=128, num_layers=3, dropout=0.2, 
                 prediction_length=1, use_attention=True):
        super(AdvancedLSTMModel, self).__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.use_attention = use_attention
        
        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
            bidirectional=True
        )
        
        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        if use_attention:
            self.attention = nn.MultiheadAttention(
                embed_dim=hidden_size * 2,  # bidirectional
                num_heads=8,
                dropout=dropout,
                batch_first=True
            )
        
        # ì™„ì „ì—°ê²°ì¸µ
        fc_input_size = hidden_size * 2 if not use_attention else hidden_size * 2
        self.fc_layers = nn.Sequential(
            nn.Linear(fc_input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, prediction_length)
        )
        
        # ë°°ì¹˜ ì •ê·œí™”
        self.batch_norm = nn.BatchNorm1d(fc_input_size)
        
    def forward(self, x):
        # LSTM
        lstm_out, (hidden, cell) = self.lstm(x)
        
        if self.use_attention:
            # ì…€í”„ ì–´í…ì…˜
            attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)
            # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…
            output = attn_output[:, -1, :]
        else:
            # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…
            output = lstm_out[:, -1, :]
        
        # ë°°ì¹˜ ì •ê·œí™”
        output = self.batch_norm(output)
        
        # ì™„ì „ì—°ê²°ì¸µ
        output = self.fc_layers(output)
        
        return output

class ModelTrainer:
    """ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.model = model.to(device)
        self.device = device
        self.train_losses = []
        self.val_losses = []
        
    def train_model(self, train_loader, val_loader, epochs=100, lr=0.001, 
                   patience=15, min_delta=1e-6):
        
        criterion = nn.MSELoss()
        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=7
        )
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        print(f"ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ - Device: {self.device}")
        
        for epoch in range(epochs):
            # í›ˆë ¨
            self.model.train()
            train_loss = 0
            for batch_x, batch_y in train_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = criterion(outputs, batch_y.squeeze(-1))
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
            
            # ê²€ì¦
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                    outputs = self.model(batch_x)
                    loss = criterion(outputs, batch_y.squeeze(-1))
                    val_loss += loss.item()
            
            train_loss /= len(train_loader)
            val_loss /= len(val_loader)
            
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
            old_lr = optimizer.param_groups[0]['lr']
            scheduler.step(val_loss)
            new_lr = optimizer.param_groups[0]['lr']
            
            # í•™ìŠµë¥  ë³€í™” ì¶œë ¥
            if old_lr != new_lr:
                print(f"   ğŸ“‰ í•™ìŠµë¥  ê°ì†Œ: {old_lr:.6f} â†’ {new_lr:.6f}")
            
            # ì¡°ê¸° ì¢…ë£Œ
            if val_loss < best_val_loss - min_delta:
                best_val_loss = val_loss
                patience_counter = 0
                # ìµœê³  ëª¨ë¸ ì €ì¥
                torch.save(self.model.state_dict(), 'best_samsung_lstm_model.pth')
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                print(f"â¹ï¸ ì¡°ê¸° ì¢…ë£Œ - Epoch {epoch+1}")
                break
                
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}, LR = {new_lr:.6f}")
        
        print(f"âœ… í›ˆë ¨ ì™„ë£Œ - ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.6f}")
        
        # ìµœê³  ëª¨ë¸ ë¡œë“œ
        self.model.load_state_dict(torch.load('best_samsung_lstm_model.pth'))
        
    def predict(self, test_loader, scaler_target):
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        self.model.eval()
        predictions = []
        actuals = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(self.device)
                output = self.model(batch_x)
                predictions.extend(output.cpu().numpy())
                actuals.extend(batch_y.numpy())
        
        # ìŠ¤ì¼€ì¼ ë³µì›
        predictions = scaler_target.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
        actuals = scaler_target.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()
        
        return predictions, actuals

def comprehensive_evaluation(y_true, y_pred, model_name="LSTM"):
    """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€"""
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    # ë°©í–¥ì„± ì •í™•ë„
    actual_diff = np.diff(y_true)
    pred_diff = np.diff(y_pred)
    direction_accuracy = np.mean(np.sign(actual_diff) == np.sign(pred_diff))
    
    # MAPE (Mean Absolute Percentage Error)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    
    # ì„ê³„ê°’ ê¸°ë°˜ ì •í™•ë„ (Â±0.1 ë‚´ì—ì„œ ì •í™•)
    threshold_accuracy = np.mean(np.abs(y_true - y_pred) <= 0.1)
    
    results = {
        'Model': model_name,
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'RÂ²': r2,
        'Direction_Accuracy': direction_accuracy,
        'MAPE': mape,
        'Threshold_Accuracy': threshold_accuracy
    }
    
    return results

def create_advanced_visualizations(data, predictions, actuals, test_dates, trainer):
    """ê³ ë„í™”ëœ ì‹œê°í™”"""
    
    # ì „ì²´ ì‹œê°í™” ì„¤ì •
    fig = make_subplots(
        rows=3, cols=2,
        subplot_titles=[
            'ì‚¼ì„±ì „ì ê°ì„± ì ìˆ˜ ì‹œê³„ì—´ (ì „ì²´ ê¸°ê°„)',
            'ì˜ˆì¸¡ vs ì‹¤ì œ (í…ŒìŠ¤íŠ¸ ê¸°ê°„)',
            'í›ˆë ¨/ê²€ì¦ ì†ì‹¤ ê³¡ì„ ',
            'ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„í¬',
            'ì›”ë³„ ê°ì„± íŠ¸ë Œë“œ',
            'ë‰´ìŠ¤ ë³¼ë¥¨ vs ê°ì„± ì ìˆ˜'
        ],
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": True}, {"secondary_y": False}]]
    )
    
    # 1. ì „ì²´ ì‹œê³„ì—´
    fig.add_trace(
        go.Scatter(x=data['date'], y=data['sentiment_mean'],
                  mode='lines', name='ì „ì²´ ê°ì„± ì ìˆ˜', line=dict(color='blue')),
        row=1, col=1
    )
    
    # 2. ì˜ˆì¸¡ vs ì‹¤ì œ
    fig.add_trace(
        go.Scatter(x=test_dates, y=actuals,
                  mode='lines+markers', name='ì‹¤ì œê°’', line=dict(color='red')),
        row=1, col=2
    )
    fig.add_trace(
        go.Scatter(x=test_dates, y=predictions,
                  mode='lines+markers', name='ì˜ˆì¸¡ê°’', line=dict(color='blue')),
        row=1, col=2
    )
    
    # 3. í›ˆë ¨ ê³¡ì„ 
    epochs = range(1, len(trainer.train_losses) + 1)
    fig.add_trace(
        go.Scatter(x=list(epochs), y=trainer.train_losses,
                  mode='lines', name='í›ˆë ¨ ì†ì‹¤', line=dict(color='blue')),
        row=2, col=1
    )
    fig.add_trace(
        go.Scatter(x=list(epochs), y=trainer.val_losses,
                  mode='lines', name='ê²€ì¦ ì†ì‹¤', line=dict(color='red')),
        row=2, col=1
    )
    
    # 4. ì˜¤ì°¨ ë¶„í¬
    errors = actuals - predictions
    fig.add_trace(
        go.Histogram(x=errors, name='ì˜ˆì¸¡ ì˜¤ì°¨', nbinsx=20),
        row=2, col=2
    )
    
    # 5. ì›”ë³„ íŠ¸ë Œë“œ
    monthly_sentiment = data.groupby(data['date'].dt.month)['sentiment_mean'].mean()
    fig.add_trace(
        go.Bar(x=list(monthly_sentiment.index), y=monthly_sentiment.values,
               name='ì›”ë³„ í‰ê·  ê°ì„±', marker_color='lightblue'),
        row=3, col=1
    )
    
    # 6. ë‰´ìŠ¤ ë³¼ë¥¨ vs ê°ì„±
    fig.add_trace(
        go.Scatter(x=data['news_count'], y=data['sentiment_mean'],
                  mode='markers', name='ë‰´ìŠ¤ë³¼ë¥¨ vs ê°ì„±',
                  marker=dict(color='green', size=4)),
        row=3, col=2
    )
    
    # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸
    fig.update_layout(
        height=1200,
        title_text="ì‚¼ì„±ì „ì ê°ì„± ë¶„ì„ ê³ ë„í™” LSTM ëª¨ë¸ - ì¢…í•© ë¶„ì„ ê²°ê³¼",
        title_x=0.5,
        showlegend=True
    )
    
    return fig

def predict_future(model, dataset, scaler_target, days=30):
    """ë¯¸ë˜ ì˜ˆì¸¡"""
    model.eval()
    
    # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    last_sequence = dataset.features[-dataset.sequence_length:]
    last_sequence = torch.FloatTensor(last_sequence).unsqueeze(0).to(next(model.parameters()).device)
    
    predictions = []
    current_sequence = last_sequence.clone()
    
    with torch.no_grad():
        for _ in range(days):
            # ì˜ˆì¸¡
            pred = model(current_sequence)
            predictions.append(pred.cpu().numpy()[0, 0])
            
            # ë‹¤ìŒ ì…ë ¥ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸ (ë‹¨ìˆœí™”ëœ ë²„ì „)
            # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²•ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
            new_features = current_sequence[0, -1, :].clone()
            new_features[0] = pred[0, 0]  # ê°ì„± ì ìˆ˜ ì—…ë°ì´íŠ¸
            
            current_sequence = torch.cat([
                current_sequence[:, 1:, :],
                new_features.unsqueeze(0).unsqueeze(0)
            ], dim=1)
    
    # ìŠ¤ì¼€ì¼ ë³µì›
    predictions = scaler_target.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
    
    return predictions

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # 1. ë°ì´í„° ë¡œë”©
    print("1ï¸âƒ£ ë°ì´í„° ë¡œë”© ì¤‘...")
    base_path = "/Users/jm/Desktop/ì¶©ë¶ëŒ€í•™êµ/ì¶©ëŒ€ 4í•™ë…„ 1í•™ê¸°/2. ë¹…ë°ì´í„°ì´í•´ì™€ë¶„ì„/íŒ€í”„ë¡œì íŠ¸/trend-prediction-model/data/processed"
    
    loader = SamsungDataLoader(base_path)
    raw_data = loader.load_all_years()
    processed_data = loader.preprocess_data()
    
    print(f"ğŸ“Š ì²˜ë¦¬ëœ ë°ì´í„° ì •ë³´:")
    print(f"   - ì´ {len(processed_data)}ì¼")
    print(f"   - ê°ì„± ì ìˆ˜ ë²”ìœ„: {processed_data['sentiment_mean'].min():.3f} ~ {processed_data['sentiment_mean'].max():.3f}")
    print(f"   - í‰ê·  ì¼ì¼ ë‰´ìŠ¤ ìˆ˜: {processed_data['news_count'].mean():.1f}ê°œ")
    
    # 2. ë°ì´í„°ì…‹ ìƒì„±
    print("\n2ï¸âƒ£ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    sequence_length = 30  # 30ì¼ ì‹œí€€ìŠ¤
    dataset = AdvancedLSTMDataset(processed_data, sequence_length=sequence_length)
    
    # ì‹œê³„ì—´ ë¶„í• 
    train_size = int(0.7 * len(dataset))
    val_size = int(0.15 * len(dataset))
    test_size = len(dataset) - train_size - val_size
    
    train_dataset = torch.utils.data.Subset(dataset, range(train_size))
    val_dataset = torch.utils.data.Subset(dataset, range(train_size, train_size + val_size))
    test_dataset = torch.utils.data.Subset(dataset, range(train_size + val_size, len(dataset)))
    
    # ë°ì´í„° ë¡œë”
    batch_size = 32
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    print(f"   - í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"   - ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}ê°œ")
    
    # 3. ëª¨ë¸ ìƒì„±
    print("\n3ï¸âƒ£ ëª¨ë¸ ìƒì„± ì¤‘...")
    input_size = len(dataset.feature_columns)
    model = AdvancedLSTMModel(
        input_size=input_size,
        hidden_size=128,
        num_layers=3,
        dropout=0.2,
        use_attention=True
    )
    
    print(f"   - ì…ë ¥ íŠ¹ì„± ìˆ˜: {input_size}")
    print(f"   - ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # 4. ëª¨ë¸ í›ˆë ¨
    print("\n4ï¸âƒ£ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
    trainer = ModelTrainer(model)
    trainer.train_model(train_loader, val_loader, epochs=100, lr=0.001)
    
    # 5. ì˜ˆì¸¡ ë° í‰ê°€
    print("\n5ï¸âƒ£ ëª¨ë¸ í‰ê°€ ì¤‘...")
    predictions, actuals = trainer.predict(test_loader, dataset.scaler_target)
    
    # í…ŒìŠ¤íŠ¸ ë‚ ì§œ ê³„ì‚°
    test_start_idx = train_size + val_size + sequence_length
    test_dates = processed_data['date'].iloc[test_start_idx:test_start_idx + len(predictions)]
    
    # í‰ê°€ ê²°ê³¼
    evaluation_results = comprehensive_evaluation(actuals, predictions, "Advanced LSTM")
    
    print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:")
    print("=" * 50)
    for key, value in evaluation_results.items():
        if key != 'Model':
            if isinstance(value, float):
                print(f"{key}: {value:.4f}")
            else:
                print(f"{key}: {value}")
    
    # 6. ì‹œê°í™”
    print("\n6ï¸âƒ£ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    
    # Plotly ìƒí˜¸ì‘ìš© ì°¨íŠ¸
    fig = create_advanced_visualizations(processed_data, predictions, actuals, test_dates, trainer)
    fig.write_html("samsung_advanced_lstm_analysis.html")
    print("   - ìƒí˜¸ì‘ìš© ì°¨íŠ¸ ì €ì¥: samsung_advanced_lstm_analysis.html")
    
    # ì •ì  ì°¨íŠ¸ (Matplotlib)
    plt.figure(figsize=(20, 12))
    
    # ì„œë¸Œí”Œë¡¯ 1: ì „ì²´ ì‹œê³„ì—´
    plt.subplot(3, 3, 1)
    plt.plot(processed_data['date'], processed_data['sentiment_mean'], alpha=0.7, linewidth=1)
    plt.title('Samsung Sentiment Score Time Series (2021-2024)')
    plt.xticks(rotation=45)
    
    # ì„œë¸Œí”Œë¡¯ 2: ì˜ˆì¸¡ vs ì‹¤ì œ
    plt.subplot(3, 3, 2)
    plt.plot(test_dates, actuals, 'r-', label='Actual', linewidth=2)
    plt.plot(test_dates, predictions, 'b--', label='Predicted', linewidth=2)
    plt.title('Prediction vs Actual (Test Period)')
    plt.legend()
    plt.xticks(rotation=45)
    
    # ì„œë¸Œí”Œë¡¯ 3: í›ˆë ¨ ê³¡ì„ 
    plt.subplot(3, 3, 3)
    plt.plot(trainer.train_losses, label='Training Loss', color='blue')
    plt.plot(trainer.val_losses, label='Validation Loss', color='red')
    plt.title('Training/Validation Loss Curve')
    plt.legend()
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    
    # ì„œë¸Œí”Œë¡¯ 4: ì˜¤ì°¨ ë¶„í¬
    plt.subplot(3, 3, 4)
    errors = actuals - predictions
    plt.hist(errors, bins=20, alpha=0.7, color='lightblue')
    plt.title('Prediction Error Distribution')
    plt.xlabel('Error')
    plt.ylabel('Frequency')
    
    # ì„œë¸Œí”Œë¡¯ 5: ì‚°ì ë„ (ì‹¤ì œ vs ì˜ˆì¸¡)
    plt.subplot(3, 3, 5)
    plt.scatter(actuals, predictions, alpha=0.6)
    plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'r--', lw=2)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Actual vs Predicted Scatter Plot')
    
    # ì„œë¸Œí”Œë¡¯ 6: ì›”ë³„ í‰ê·  ê°ì„±
    plt.subplot(3, 3, 6)
    monthly_sentiment = processed_data.groupby(processed_data['date'].dt.month)['sentiment_mean'].mean()
    plt.bar(monthly_sentiment.index, monthly_sentiment.values, color='lightgreen')
    plt.title('Monthly Average Sentiment Score')
    plt.xlabel('Month')
    plt.ylabel('Average Sentiment Score')
    
    # ì„œë¸Œí”Œë¡¯ 7: ë‰´ìŠ¤ ë³¼ë¥¨ vs ê°ì„±
    plt.subplot(3, 3, 7)
    plt.scatter(processed_data['news_count'], processed_data['sentiment_mean'], alpha=0.5)
    plt.xlabel('Daily News Count')
    plt.ylabel('Sentiment Score')
    plt.title('News Volume vs Sentiment Score')
    
    # ì„œë¸Œí”Œë¡¯ 8: ì—°ë„ë³„ ê°ì„± íŠ¸ë Œë“œ
    plt.subplot(3, 3, 8)
    yearly_sentiment = processed_data.groupby(processed_data['date'].dt.year)['sentiment_mean'].mean()
    plt.bar(yearly_sentiment.index, yearly_sentiment.values, color='orange')
    plt.title('Yearly Average Sentiment Score')
    plt.xlabel('Year')
    plt.ylabel('Average Sentiment Score')
    
    # ì„œë¸Œí”Œë¡¯ 9: ë³€ë™ì„± ë¶„ì„
    plt.subplot(3, 3, 9)
    plt.plot(processed_data['date'], processed_data['volatility_7'], color='purple', alpha=0.7)
    plt.title('7-Day Moving Volatility')
    plt.xticks(rotation=45)
    plt.ylabel('Volatility')
    
    plt.tight_layout()
    plt.savefig('samsung_advanced_lstm_static_analysis.png', dpi=300, bbox_inches='tight')
    print("   - ì •ì  ì°¨íŠ¸ ì €ì¥: samsung_advanced_lstm_static_analysis.png")
    
    # 7. ë¯¸ë˜ ì˜ˆì¸¡
    print("\n7ï¸âƒ£ ë¯¸ë˜ 30ì¼ ì˜ˆì¸¡ ì¤‘...")
    future_predictions = predict_future(model, dataset, dataset.scaler_target, days=30)
    
    # ë¯¸ë˜ ë‚ ì§œ ìƒì„±
    last_date = processed_data['date'].max()
    future_dates = [last_date + timedelta(days=i+1) for i in range(30)]
    
    # ë¯¸ë˜ ì˜ˆì¸¡ ì‹œê°í™”
    plt.figure(figsize=(15, 8))
    
    # ìµœê·¼ 60ì¼ê³¼ ë¯¸ë˜ 30ì¼ ì‹œê°í™”
    recent_data = processed_data.tail(60)
    
    plt.plot(recent_data['date'], recent_data['sentiment_mean'], 
             'b-', label='Actual Sentiment Score', linewidth=2)
    plt.plot(future_dates, future_predictions, 
             'r--', label='Future Prediction', linewidth=2, marker='o', markersize=4)
    
    # ì‹ ë¢°êµ¬ê°„ ì¶”ê°€ (ë‹¨ìˆœí™”ëœ ë²„ì „)
    std_error = np.std(predictions - actuals)
    confidence_interval = 1.96 * std_error
    plt.fill_between(future_dates, 
                     future_predictions - confidence_interval,
                     future_predictions + confidence_interval,
                     alpha=0.3, color='red', label='95% Confidence Interval')
    
    plt.axvline(x=last_date, color='gray', linestyle=':', alpha=0.7, label='Prediction Start')
    plt.title('Samsung Sentiment Score 30-Day Future Prediction', fontsize=16)
    plt.xlabel('Date')
    plt.ylabel('Sentiment Score')
    plt.legend()
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('samsung_future_prediction_30days.png', dpi=300, bbox_inches='tight')
    print("   - ë¯¸ë˜ ì˜ˆì¸¡ ì°¨íŠ¸ ì €ì¥: samsung_future_prediction_30days.png")
    
    # 8. ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    print("\n8ï¸âƒ£ ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ë‹¨ìˆœí™”ëœ ë²„ì „)
    feature_importance = {}
    for i, feature in enumerate(dataset.feature_columns):
        # íŠ¹ì„±ë³„ í‘œì¤€í¸ì°¨ë¥¼ ì¤‘ìš”ë„ë¡œ ì‚¬ìš© (ë‹¨ìˆœí™”)
        importance = np.std(dataset.features[:, i])
        feature_importance[feature] = importance
    
    # ì •ë ¬
    sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
    
    # ì„±ëŠ¥ ë¹„êµ (ì´ì „ ê²°ê³¼ì™€ ë¹„êµ)
    performance_comparison = {
        'ì´ì „ ê¸°ë³¸ LSTM': {'RÂ²': 0.096, 'MAE': 0.15},
        'í˜„ì¬ ê³ ë„í™” LSTM': {'RÂ²': evaluation_results['RÂ²'], 'MAE': evaluation_results['MAE']}
    }
    
    # ë¦¬í¬íŠ¸ ì‘ì„±
    report = f"""
# ì‚¼ì„±ì „ì ê°ì„± ë¶„ì„ ê³ ë„í™” LSTM ëª¨ë¸ - ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ“Š í”„ë¡œì íŠ¸ ê°œìš”
- **ë¶„ì„ ëŒ€ìƒ**: ì‚¼ì„±ì „ì ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ (2021-2024)
- **ë°ì´í„° í¬ì¸íŠ¸**: {len(processed_data):,}ì¼
- **ëª¨ë¸**: Advanced LSTM with Attention
- **ë¶„ì„ ì¼ì**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼')}

## ğŸ¯ ëª¨ë¸ ì„±ëŠ¥
### ì£¼ìš” ì§€í‘œ
- **RÂ² Score**: {evaluation_results['RÂ²']:.4f}
- **RMSE**: {evaluation_results['RMSE']:.4f}
- **MAE**: {evaluation_results['MAE']:.4f}
- **ë°©í–¥ì„± ì •í™•ë„**: {evaluation_results['Direction_Accuracy']:.1%}
- **MAPE**: {evaluation_results['MAPE']:.2f}%
- **ì„ê³„ê°’ ì •í™•ë„**: {evaluation_results['Threshold_Accuracy']:.1%}

### ì„±ëŠ¥ ê°œì„ ë„
"""

    for model_name, metrics in performance_comparison.items():
        report += f"- **{model_name}**: RÂ² = {metrics['RÂ²']:.4f}, MAE = {metrics['MAE']:.4f}\n"
    
    improvement_r2 = evaluation_results['RÂ²'] - 0.096
    improvement_pct = (improvement_r2 / 0.096) * 100 if 0.096 != 0 else 0
    
    report += f"\n**ê°œì„ ë„**: RÂ² {improvement_r2:+.4f} ({improvement_pct:+.1f}%)\n"
    
    report += f"""

## ğŸ“ˆ ë°ì´í„° íŠ¹ì„± ë¶„ì„
### ê¸°ë³¸ í†µê³„
- **ê°ì„± ì ìˆ˜ ë²”ìœ„**: {processed_data['sentiment_mean'].min():.3f} ~ {processed_data['sentiment_mean'].max():.3f}
- **í‰ê·  ê°ì„± ì ìˆ˜**: {processed_data['sentiment_mean'].mean():.3f}
- **í‘œì¤€í¸ì°¨**: {processed_data['sentiment_mean'].std():.3f}
- **í‰ê·  ì¼ì¼ ë‰´ìŠ¤ ìˆ˜**: {processed_data['news_count'].mean():.1f}ê°œ

### íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 5ê°œ)
"""

    for i, (feature, importance) in enumerate(sorted_features[:5]):
        report += f"{i+1}. **{feature}**: {importance:.4f}\n"
    
    report += f"""

## ğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼
### 30ì¼ ì˜ˆì¸¡ ìš”ì•½
- **ì˜ˆì¸¡ ì‹œì‘ì¼**: {last_date.strftime('%Y-%m-%d')}
- **ì˜ˆì¸¡ ì¢…ë£Œì¼**: {future_dates[-1].strftime('%Y-%m-%d')}
- **ì˜ˆì¸¡ í‰ê· ê°’**: {np.mean(future_predictions):.3f}
- **ì˜ˆì¸¡ ë³€ë™ì„±**: {np.std(future_predictions):.3f}
- **íŠ¸ë Œë“œ**: {"ìƒìŠ¹" if future_predictions[-1] > future_predictions[0] else "í•˜ë½" if future_predictions[-1] < future_predictions[0] else "íš¡ë³´"}

### ì£¼ìš” ì¸ì‚¬ì´íŠ¸
"""

    # íŠ¸ë Œë“œ ë¶„ì„
    if future_predictions[-1] > future_predictions[0]:
        trend_msg = "í–¥í›„ 30ì¼ ë™ì•ˆ ì‚¼ì„±ì „ìì— ëŒ€í•œ ê°ì„±ì´ ì ì§„ì ìœ¼ë¡œ ê°œì„ ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
    elif future_predictions[-1] < future_predictions[0]:
        trend_msg = "í–¥í›„ 30ì¼ ë™ì•ˆ ì‚¼ì„±ì „ìì— ëŒ€í•œ ê°ì„±ì´ ë‹¤ì†Œ í•˜ë½í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
    else:
        trend_msg = "í–¥í›„ 30ì¼ ë™ì•ˆ ì‚¼ì„±ì „ìì— ëŒ€í•œ ê°ì„±ì´ í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤."
    
    report += f"- {trend_msg}\n"
    
    # ë³€ë™ì„± ë¶„ì„
    volatility_level = "ë†’ìŒ" if np.std(future_predictions) > processed_data['sentiment_mean'].std() else "ë³´í†µ" if np.std(future_predictions) > processed_data['sentiment_mean'].std() * 0.5 else "ë‚®ìŒ"
    report += f"- ì˜ˆìƒ ë³€ë™ì„±ì€ **{volatility_level}** ìˆ˜ì¤€ì…ë‹ˆë‹¤.\n"
    
    # ì—°ë„ë³„ ë¹„êµ
    yearly_avg = processed_data.groupby(processed_data['date'].dt.year)['sentiment_mean'].mean()
    current_year_avg = yearly_avg.iloc[-1] if len(yearly_avg) > 0 else processed_data['sentiment_mean'].mean()
    
    if np.mean(future_predictions) > current_year_avg:
        year_comparison = "ì‘ë…„ ëŒ€ë¹„ ê°œì„ ëœ"
    elif np.mean(future_predictions) < current_year_avg:
        year_comparison = "ì‘ë…„ ëŒ€ë¹„ í•˜ë½í•œ"
    else:
        year_comparison = "ì‘ë…„ê³¼ ìœ ì‚¬í•œ"
    
    report += f"- ì˜ˆì¸¡ëœ ê°ì„± ìˆ˜ì¤€ì€ {year_comparison} ìˆ˜ì¤€ì…ë‹ˆë‹¤.\n"
    
    report += f"""

## ğŸ” ëª¨ë¸ ë¶„ì„
### ëª¨ë¸ ì•„í‚¤í…ì²˜
- **ì…ë ¥ íŠ¹ì„±**: {len(dataset.feature_columns)}ê°œ
- **ì‹œí€€ìŠ¤ ê¸¸ì´**: {sequence_length}ì¼
- **LSTM ë ˆì´ì–´**: 3ì¸µ (ì–‘ë°©í–¥)
- **ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**: ë©€í‹°í—¤ë“œ ì–´í…ì…˜ (8í—¤ë“œ)
- **ì •ê·œí™”**: Dropout (0.2), ë°°ì¹˜ ì •ê·œí™”

### í›ˆë ¨ ê³¼ì •
- **ì´ ì—í¬í¬**: {len(trainer.train_losses)}
- **ìµœì¢… í›ˆë ¨ ì†ì‹¤**: {trainer.train_losses[-1]:.6f}
- **ìµœì¢… ê²€ì¦ ì†ì‹¤**: {trainer.val_losses[-1]:.6f}
- **ì¡°ê¸° ì¢…ë£Œ**: {"ì ìš©ë¨" if len(trainer.train_losses) < 100 else "ë¯¸ì ìš©"}

## ğŸ“‹ ê²°ë¡  ë° ì œì–¸
### ì£¼ìš” ì„±ê³¼
1. **ëª¨ë¸ ì„±ëŠ¥ ê°œì„ **: ê¸°ì¡´ ëŒ€ë¹„ RÂ² ì ìˆ˜ í–¥ìƒ
2. **ë‹¤ì°¨ì› ë¶„ì„**: 11ê°œ íŠ¹ì„±ì„ í™œìš©í•œ ì¢…í•©ì  ë¶„ì„
3. **ì‹¤ìš©ì  ì˜ˆì¸¡**: ë°©í–¥ì„± ì •í™•ë„ {evaluation_results['Direction_Accuracy']:.1%} ë‹¬ì„±

### í•œê³„ì 
1. **ì˜ˆì¸¡ ì •í™•ë„**: ì—¬ì „íˆ RÂ² < 0.5 ìˆ˜ì¤€
2. **ë…¸ì´ì¦ˆ ì˜í–¥**: ì¼ë³„ ë°ì´í„°ì˜ ë†’ì€ ë³€ë™ì„±
3. **ì™¸ë¶€ ìš”ì¸**: ì£¼ê°€, ê²½ì œì§€í‘œ ë“± ë¯¸ë°˜ì˜

### ê°œì„  ë°©ì•ˆ
1. **ë°ì´í„° í™•ì¥**: ì†Œì…œë¯¸ë””ì–´, ì¬ë¬´ë°ì´í„° í†µí•©
2. **ì‹œê°„ ë‹¨ìœ„ ì¡°ì •**: ì£¼ê°„/ì›”ê°„ ë‹¨ìœ„ ë¶„ì„ ê³ ë ¤
3. **ì•™ìƒë¸” ëª¨ë¸**: ì—¬ëŸ¬ ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
4. **ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸**: ìƒˆë¡œìš´ ë‰´ìŠ¤ ë°ì´í„° ìë™ ë°˜ì˜

## ğŸ“Š í™œìš© ë°©ì•ˆ
### ë¹„ì¦ˆë‹ˆìŠ¤ ì ìš©
- **ë§ˆì¼€íŒ… íƒ€ì´ë°**: ê°ì„± ê°œì„  ì‹œì ì— ë§ˆì¼€íŒ… ì§‘ì¤‘
- **ìœ„ê¸° ê´€ë¦¬**: ê°ì„± ê¸‰ë½ ì˜ˆìƒ ì‹œ ì„ ì œì  ëŒ€ì‘
- **íˆ¬ì ì°¸ê³ **: ê°ì„± íŠ¸ë Œë“œë¥¼ íˆ¬ì ì˜ì‚¬ê²°ì •ì— ì°¸ê³ 

### ê¸°ìˆ ì  ë°œì „
- **ëª¨ë¸ ê³ ë„í™”**: Transformer, BERT ë“± ìµœì‹  ëª¨ë¸ ì ìš©
- **ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ**: ë‰´ìŠ¤ ìˆ˜ì§‘ë¶€í„° ì˜ˆì¸¡ê¹Œì§€ ìë™í™”
- **ë‹¤ê¸°ì—… í™•ì¥**: ë‹¤ë¥¸ ê¸°ì—…ìœ¼ë¡œ ëª¨ë¸ í™•ì¥ ì ìš©

---
*ë³¸ ë¦¬í¬íŠ¸ëŠ” AI ê¸°ë°˜ ê°ì„± ë¶„ì„ ëª¨ë¸ì˜ ê²°ê³¼ì´ë©°, íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹Œ ì°¸ê³  ìë£Œë¡œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.*
"""

    # ë¦¬í¬íŠ¸ ì €ì¥
    with open('samsung_advanced_lstm_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("   - ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: samsung_advanced_lstm_report.md")
    
    # 9. ìš”ì•½ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ‰ ì‚¼ì„±ì „ì ê°ì„± ë¶„ì„ ê³ ë„í™” LSTM ëª¨ë¸ ë¶„ì„ ì™„ë£Œ!")
    print("="*60)
    print(f"ğŸ“ˆ ìµœì¢… ì„±ëŠ¥: RÂ² = {evaluation_results['RÂ²']:.4f}")
    print(f"ğŸ¯ ë°©í–¥ì„± ì •í™•ë„: {evaluation_results['Direction_Accuracy']:.1%}")
    print(f"ğŸ“Š ìƒì„±ëœ íŒŒì¼:")
    print("   - samsung_advanced_lstm_analysis.html (ìƒí˜¸ì‘ìš© ì°¨íŠ¸)")
    print("   - samsung_advanced_lstm_static_analysis.png (ì •ì  ì°¨íŠ¸)")
    print("   - samsung_future_prediction_30days.png (ë¯¸ë˜ ì˜ˆì¸¡)")
    print("   - samsung_advanced_lstm_report.md (ìƒì„¸ ë¦¬í¬íŠ¸)")
    print("   - best_samsung_lstm_model.pth (í›ˆë ¨ëœ ëª¨ë¸)")
    
    # ê°œì„  ì œì•ˆ
    if evaluation_results['RÂ²'] < 0.3:
        print("\nğŸ’¡ ì„±ëŠ¥ ê°œì„  ì œì•ˆ:")
        print("   1. ì£¼ê°„ ë‹¨ìœ„ë¡œ ë°ì´í„° ì§‘ê³„í•˜ì—¬ ë…¸ì´ì¦ˆ ê°ì†Œ")
        print("   2. ì£¼ê°€, ê±°ë˜ëŸ‰ ë“± ì™¸ë¶€ ë°ì´í„° ì¶”ê°€")
        print("   3. ê°ì„± ë¶„ì„ í’ˆì§ˆ ê°œì„  (KoBERT ì¬í•™ìŠµ)")
        print("   4. ì•™ìƒë¸” ëª¨ë¸ ì ìš©")
    
    return {
        'model': model,
        'dataset': dataset,
        'evaluation_results': evaluation_results,
        'predictions': predictions,
        'actuals': actuals,
        'future_predictions': future_predictions,
        'processed_data': processed_data
    }

if __name__ == "__main__":
    # ì‹¤í–‰
    results = main()
    print("\nâœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")