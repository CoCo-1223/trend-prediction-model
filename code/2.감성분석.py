"""
ë¹…ì¹´ì¸ì¦ˆ ë‰´ìŠ¤ ë°ì´í„° ê°ì„±ë¶„ì„ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- ì¡°ì›ì˜ ê°ì„±ë¶„ì„ ì½”ë“œì™€ ë™ì¼í•œ ê²°ê³¼ ìƒì„±
- ì…ë ¥: ë¹…ì¹´ì¸ì¦ˆ ì—‘ì…€ íŒŒì¼ (NewsResult_20XX0101-20XX1231.xlsx)
- ì¶œë ¥: samsung_sentiment_{year}.csv, apple_sentiment_{year}.csv
"""

import pandas as pd
import numpy as np
import os
import glob
from datetime import datetime
from transformers import pipeline
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

class BigKindsSentimentProcessor:
    def __init__(self, data_dir, output_dir="./data/processed"):
        """
        ë¹…ì¹´ì¸ì¦ˆ ê°ì„±ë¶„ì„ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            data_dir (str): ë¹…ì¹´ì¸ì¦ˆ ì—‘ì…€ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
            output_dir (str): ê²°ê³¼ CSV íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.batch_size = 32
        self.model_id = "snunlp/KR-FinBert-SC"
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # FinBERT ê°ì„±ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        print("ğŸ¤– FinBERT ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model=self.model_id,
            tokenizer=self.model_id,
            device=-1,  # GPU ì‚¬ìš© ì‹œ 0ìœ¼ë¡œ ë³€ê²½
            top_k=None  # ëª¨ë“  ë¼ë²¨ í™•ë¥  ë°˜í™˜
        )
        print("âœ… FinBERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
    def find_excel_files(self):
        """ë¹…ì¹´ì¸ì¦ˆ ì—‘ì…€ íŒŒì¼ë“¤ ì°¾ê¸°"""
        pattern = os.path.join(self.data_dir, "NewsResult_*-*.xlsx")
        files = glob.glob(pattern)
        
        if not files:
            print(f"âŒ {self.data_dir}ì—ì„œ ë¹…ì¹´ì¸ì¦ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ì°¾ëŠ” íŒ¨í„´: NewsResult_*-*.xlsx")
            return []
        
        # ì—°ë„ë³„ë¡œ ì •ë ¬
        files.sort()
        print(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼ë“¤:")
        for file in files:
            print(f"   - {os.path.basename(file)}")
        
        return files
    
    def extract_year_from_filename(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ì—°ë„ ì¶”ì¶œ"""
        basename = os.path.basename(filename)
        # NewsResult_20230101-20231231.xlsx -> 2023 ì¶”ì¶œ
        try:
            year = basename.split('_')[1][:4]
            return year
        except:
            # ë‹¤ë¥¸ í˜•ì‹ì˜ íŒŒì¼ëª… ì²˜ë¦¬
            import re
            year_match = re.search(r'20\d{2}', basename)
            if year_match:
                return year_match.group()
            return "unknown"
    
    def load_and_clean_excel(self, file_path):
        """ì—‘ì…€ íŒŒì¼ ë¡œë“œ ë° ì •ì œ"""
        print(f"\nğŸ“Š {os.path.basename(file_path)} ì²˜ë¦¬ ì¤‘...")
        
        try:
            # ì—‘ì…€ íŒŒì¼ ë¡œë“œ
            df = pd.read_excel(file_path)
            print(f"   - ì›ë³¸ ë°ì´í„°: {len(df)}ê±´")
            
            # ì»¬ëŸ¼ëª… ê³µë°± ì œê±°
            df.columns = df.columns.str.strip()
            
            print(f"   - ì»¬ëŸ¼: {list(df.columns)}")
            
            # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ë§¤í•‘
            required_columns = ['ì¼ì', 'ì œëª©', 'í‚¤ì›Œë“œ']
            column_mapping = {}
            
            for col in df.columns:
                col_lower = col.lower()
                if any(date_word in col_lower for date_word in ['ì¼ì', 'ë‚ ì§œ', 'date', 'ê¸°ì‚¬ì¼ì']):
                    column_mapping[col] = 'ì¼ì'
                elif any(title_word in col_lower for title_word in ['ì œëª©', 'title', 'ê¸°ì‚¬ì œëª©']):
                    column_mapping[col] = 'ì œëª©'
                elif any(keyword_word in col_lower for keyword_word in ['í‚¤ì›Œë“œ', 'keyword', 'ì£¼ìš”í‚¤ì›Œë“œ']):
                    column_mapping[col] = 'í‚¤ì›Œë“œ'
                elif any(content_word in col_lower for content_word in ['ë³¸ë¬¸', 'content', 'ë‚´ìš©']):
                    column_mapping[col] = 'ë³¸ë¬¸'
                elif any(media_word in col_lower for media_word in ['ì–¸ë¡ ì‚¬', 'ë§¤ì²´', 'media']):
                    column_mapping[col] = 'ì–¸ë¡ ì‚¬'
            
            # ì»¬ëŸ¼ëª… ë³€ê²½
            df = df.rename(columns=column_mapping)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                print(f"   âš ï¸ ëˆ„ë½ëœ í•„ìˆ˜ ì»¬ëŸ¼: {missing_columns}")
                return None
            
            # ë°ì´í„° íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì¡°ì› ì½”ë“œì™€ ë™ì¼)
            for col in df.columns:
                df[col] = df[col].astype(str)
            
            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            df = df.dropna(subset=['ì¼ì', 'ì œëª©'])
            df['í‚¤ì›Œë“œ'] = df['í‚¤ì›Œë“œ'].fillna('')
            if 'ë³¸ë¬¸' in df.columns:
                df['ë³¸ë¬¸'] = df['ë³¸ë¬¸'].fillna('')
            
            print(f"   - ì •ì œ í›„ ë°ì´í„°: {len(df)}ê±´")
            
            return df
            
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def parse_and_format_date(self, df):
        """ë‚ ì§œ íŒŒì‹± ë° í¬ë§·íŒ… (ì¡°ì› ì½”ë“œì™€ ë™ì¼)"""
        print("   - ë‚ ì§œ ì²˜ë¦¬ ì¤‘...")
        
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            df["ì¼ì"] = pd.to_datetime(df["ì¼ì"], errors="coerce")
            
            # NaT ê°’ ì œê±°
            before_count = len(df)
            df = df.dropna(subset=["ì¼ì"])
            after_count = len(df)
            
            if before_count != after_count:
                print(f"     ë‚ ì§œ ì˜¤ë¥˜ë¡œ ì œê±°ëœ ë°ì´í„°: {before_count - after_count}ê±´")
            
            # YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            df["ì¼ì"] = df["ì¼ì"].dt.strftime("%Y-%m-%d")
            
            print(f"     ë‚ ì§œ ë²”ìœ„: {df['ì¼ì'].min()} ~ {df['ì¼ì'].max()}")
            
        except Exception as e:
            print(f"     ë‚ ì§œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            # ë‚ ì§œ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
            pass
        
        return df
    
    def create_text_and_classify_company(self, df):
        """í…ìŠ¤íŠ¸ ê²°í•© ë° ê¸°ì—… ë¶„ë¥˜ (ì¡°ì› ì½”ë“œì™€ ë™ì¼)"""
        print("   - í…ìŠ¤íŠ¸ ê²°í•© ë° ê¸°ì—… ë¶„ë¥˜ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ê²°í•©
        df["text"] = df["ì œëª©"].fillna("") + " í‚¤ì›Œë“œ:" + df["í‚¤ì›Œë“œ"].fillna("")
        
        # ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if "ë³¸ë¬¸" in df.columns:
            df["text"] = df["text"] + " " + df["ë³¸ë¬¸"].fillna("")
        
        # ê¸°ì—… ë¶„ë¥˜ í•¨ìˆ˜ (ì¡°ì› ì½”ë“œì™€ ë™ì¼)
        def company_of(txt: str) -> str:
            if pd.isna(txt):
                return None
            
            t = str(txt).lower()
            
            # ì‚¼ì„± í‚¤ì›Œë“œ
            samsung_keywords = ['ì‚¼ì„±ì „ì', 'ì‚¼ì„±', 'samsung']
            # ì• í”Œ í‚¤ì›Œë“œ  
            apple_keywords = ['ì• í”Œ', 'apple']
            
            samsung_found = any(keyword in t for keyword in samsung_keywords)
            apple_found = any(keyword in t for keyword in apple_keywords)
            
            if samsung_found and not apple_found:
                return "samsung"
            elif apple_found and not samsung_found:
                return "apple"
            elif samsung_found and apple_found:
                # ë‘˜ ë‹¤ ìˆëŠ” ê²½ìš° ë” ë§ì´ ì–¸ê¸‰ëœ ìª½ìœ¼ë¡œ
                samsung_count = sum(t.count(keyword) for keyword in samsung_keywords)
                apple_count = sum(t.count(keyword) for keyword in apple_keywords)
                return "samsung" if samsung_count >= apple_count else "apple"
            else:
                return None
        
        # ê¸°ì—… ë¶„ë¥˜ ì ìš©
        df["ê¸°ì—…"] = df["text"].apply(company_of)
        
        # ê¸°ì—…ì´ ë¶„ë¥˜ëœ ë°ì´í„°ë§Œ ìœ ì§€
        before_count = len(df)
        df = df.dropna(subset=["ê¸°ì—…"]).reset_index(drop=True)
        after_count = len(df)
        
        samsung_count = len(df[df["ê¸°ì—…"] == "samsung"])
        apple_count = len(df[df["ê¸°ì—…"] == "apple"])
        
        print(f"     ê¸°ì—… ë¶„ë¥˜ ê²°ê³¼: ì‚¼ì„± {samsung_count}ê±´, ì• í”Œ {apple_count}ê±´")
        print(f"     ë¶„ë¥˜ë˜ì§€ ì•Šì€ ë°ì´í„° ì œê±°: {before_count - after_count}ê±´")
        
        return df
    
    def perform_sentiment_analysis(self, df):
        """ê°ì„±ë¶„ì„ ìˆ˜í–‰ (ì¡°ì› ì½”ë“œì™€ ë™ì¼)"""
        print("   - ê°ì„±ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        
        # ê°ì„± ì ìˆ˜ ë³€í™˜ í•¨ìˆ˜ (ì¡°ì› ì½”ë“œì™€ ë™ì¼)
        def to_score(probs: dict) -> float:
            neg = probs.get("negative", 0.0)
            neu = probs.get("neutral", 0.0) 
            pos = probs.get("positive", 0.0)
            
            # ì¤‘ë¦½=3ì , ê¸ì •Â·ë¶€ì •ì˜ ì°¨ì´ì— ë”°ë¼ Â±2ì  ë²”ìœ„ ë§¤í•‘
            score = 3.0 + 2.0 * (pos - neg)
            return max(1.0, min(5.0, score))
        
        labels, scores = [], []
        texts = df["text"].tolist()
        
        # ë°°ì¹˜ë³„ ê°ì„±ë¶„ì„
        for i in tqdm(range(0, len(texts), self.batch_size), desc="ê°ì„±ë¶„ì„"):
            batch = texts[i : i + self.batch_size]
            
            try:
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                batch = [str(text)[:512] for text in batch]
                
                # ê°ì„±ë¶„ì„ ìˆ˜í–‰
                outputs = self.sentiment_pipeline(batch, truncation=True, max_length=512)
                
                for out in outputs:
                    # ê²°ê³¼ ì²˜ë¦¬ (list ë˜ëŠ” dict í˜•íƒœ ëª¨ë‘ ì²˜ë¦¬)
                    if isinstance(out, list):
                        prob_map = {x["label"]: x["score"] for x in out}
                    else:
                        prob_map = out
                    
                    # ìµœê³  í™•ë¥  ë¼ë²¨
                    best_label = max(prob_map, key=prob_map.get)
                    labels.append(best_label)
                    scores.append(to_score(prob_map))
                    
            except Exception as e:
                print(f"     ë°°ì¹˜ {i//self.batch_size + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ì¤‘ë¦½ìœ¼ë¡œ ì²˜ë¦¬
                for _ in range(len(batch)):
                    labels.append("neutral")
                    scores.append(3.0)
        
        df["ê°ì •ë¼ë²¨"] = labels
        df["ê°ì •ì ìˆ˜"] = scores
        
        # ê°ì„±ë¶„ì„ ê²°ê³¼ ìš”ì•½
        sentiment_dist = df["ê°ì •ë¼ë²¨"].value_counts()
        print(f"     ê°ì„±ë¶„ì„ ê²°ê³¼:")
        for sentiment, count in sentiment_dist.items():
            print(f"       {sentiment}: {count}ê±´")
        print(f"     í‰ê·  ê°ì •ì ìˆ˜: {df['ê°ì •ì ìˆ˜'].mean():.2f}")
        
        return df
    
    def save_company_files(self, df, year):
        """ê¸°ì—…ë³„ CSV íŒŒì¼ ì €ì¥ (ì¡°ì› ì½”ë“œì™€ ë™ì¼)"""
        print("   - ê¸°ì—…ë³„ íŒŒì¼ ì €ì¥ ì¤‘...")
        
        # ë‚ ì§œìˆœ ì •ë ¬
        df = df.sort_values("ì¼ì")
        
        saved_files = []
        
        # ê¸°ì—…ë³„ë¡œ ì €ì¥
        for company in ["samsung", "apple"]:
            company_data = df[df["ê¸°ì—…"] == company][
                ["ì¼ì", "ì œëª©", "í‚¤ì›Œë“œ", "ê°ì •ë¼ë²¨", "ê°ì •ì ìˆ˜"]
            ]
            
            if len(company_data) > 0:
                output_path = os.path.join(self.output_dir, f"{company}_sentiment_{year}.csv")
                company_data.to_csv(output_path, index=False, encoding="utf-8-sig")
                
                print(f"     âœ… {os.path.basename(output_path)}: {len(company_data)}ê±´ ì €ì¥")
                saved_files.append(output_path)
            else:
                print(f"     âš ï¸ {company} ë°ì´í„° ì—†ìŒ")
        
        return saved_files
    
    def process_single_file(self, file_path):
        """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬"""
        year = self.extract_year_from_filename(file_path)
        
        # 1. ì—‘ì…€ íŒŒì¼ ë¡œë“œ ë° ì •ì œ
        df = self.load_and_clean_excel(file_path)
        if df is None:
            return []
        
        # 2. ë‚ ì§œ ì²˜ë¦¬
        df = self.parse_and_format_date(df)
        
        # 3. í…ìŠ¤íŠ¸ ê²°í•© ë° ê¸°ì—… ë¶„ë¥˜
        df = self.create_text_and_classify_company(df)
        
        if len(df) == 0:
            print("   âš ï¸ ë¶„ë¥˜ëœ ê¸°ì—… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 4. ê°ì„±ë¶„ì„
        df = self.perform_sentiment_analysis(df)
        
        # 5. ê²°ê³¼ ì €ì¥
        saved_files = self.save_company_files(df, year)
        
        return saved_files
    
    def process_all_files(self):
        """ëª¨ë“  ë¹…ì¹´ì¸ì¦ˆ íŒŒì¼ ì²˜ë¦¬"""
        print("ğŸš€ ë¹…ì¹´ì¸ì¦ˆ ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ì²˜ë¦¬ ì‹œì‘")
        print("=" * 60)
        
        # íŒŒì¼ ì°¾ê¸°
        excel_files = self.find_excel_files()
        if not excel_files:
            return []
        
        all_saved_files = []
        
        # ê° íŒŒì¼ ì²˜ë¦¬
        for file_path in excel_files:
            try:
                saved_files = self.process_single_file(file_path)
                all_saved_files.extend(saved_files)
                
            except Exception as e:
                print(f"âŒ {os.path.basename(file_path)} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("ğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
        print("=" * 60)
        
        if all_saved_files:
            print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼ ({len(all_saved_files)}ê°œ):")
            for file_path in all_saved_files:
                file_size = os.path.getsize(file_path) / 1024  # KB
                print(f"   - {os.path.basename(file_path)} ({file_size:.1f} KB)")
            
            print(f"\nğŸ“‚ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(self.output_dir)}")
            
            # ë°ì´í„° í†µê³„
            print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:")
            samsung_files = [f for f in all_saved_files if 'samsung' in f]
            apple_files = [f for f in all_saved_files if 'apple' in f]
            
            print(f"   - ì‚¼ì„± ë°ì´í„°: {len(samsung_files)}ê°œ ì—°ë„")
            print(f"   - ì• í”Œ ë°ì´í„°: {len(apple_files)}ê°œ ì—°ë„")
            
            # ê° íŒŒì¼ì˜ ë°ì´í„° ìˆ˜ í™•ì¸
            total_records = 0
            for file_path in all_saved_files:
                try:
                    df = pd.read_csv(file_path)
                    records = len(df)
                    total_records += records
                    company = "ì‚¼ì„±" if "samsung" in file_path else "ì• í”Œ"
                    year = file_path.split("_")[-1].replace(".csv", "")
                    print(f"     {company} {year}: {records:,}ê±´")
                except:
                    pass
            
            print(f"   - ì´ ê°ì„±ë¶„ì„ ë ˆì½”ë“œ: {total_records:,}ê±´")
            
        else:
            print("âŒ ìƒì„±ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        return all_saved_files
    
    def verify_output_format(self):
        """ì¶œë ¥ íŒŒì¼ í˜•ì‹ ê²€ì¦"""
        print("\nğŸ” ì¶œë ¥ íŒŒì¼ í˜•ì‹ ê²€ì¦ ì¤‘...")
        
        output_files = glob.glob(os.path.join(self.output_dir, "*_sentiment_*.csv"))
        
        if not output_files:
            print("âŒ ê²€ì¦í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        all_valid = True
        required_columns = ['ì¼ì', 'ì œëª©', 'í‚¤ì›Œë“œ', 'ê°ì •ë¼ë²¨', 'ê°ì •ì ìˆ˜']
        
        for file_path in output_files:
            try:
                df = pd.read_csv(file_path)
                
                # ì»¬ëŸ¼ í™•ì¸
                missing_cols = [col for col in required_columns if col not in df.columns]
                if missing_cols:
                    print(f"âŒ {os.path.basename(file_path)}: ëˆ„ë½ ì»¬ëŸ¼ {missing_cols}")
                    all_valid = False
                    continue
                
                # ë°ì´í„° íƒ€ì… í™•ì¸
                if not pd.api.types.is_numeric_dtype(df['ê°ì •ì ìˆ˜']):
                    print(f"âŒ {os.path.basename(file_path)}: ê°ì •ì ìˆ˜ê°€ ìˆ«ìê°€ ì•„ë‹˜")
                    all_valid = False
                    continue
                
                # ê°ì •ì ìˆ˜ ë²”ìœ„ í™•ì¸
                if df['ê°ì •ì ìˆ˜'].min() < 1 or df['ê°ì •ì ìˆ˜'].max() > 5:
                    print(f"âŒ {os.path.basename(file_path)}: ê°ì •ì ìˆ˜ ë²”ìœ„ ì˜¤ë¥˜")
                    all_valid = False
                    continue
                
                print(f"âœ… {os.path.basename(file_path)}: í˜•ì‹ ì˜¬ë°”ë¦„ ({len(df)}ê±´)")
                
            except Exception as e:
                print(f"âŒ {os.path.basename(file_path)}: ê²€ì¦ ì‹¤íŒ¨ - {e}")
                all_valid = False
        
        if all_valid:
            print("ğŸ‰ ëª¨ë“  íŒŒì¼ì˜ í˜•ì‹ì´ ì˜¬ë°”ë¦…ë‹ˆë‹¤!")
        else:
            print("âš ï¸ ì¼ë¶€ íŒŒì¼ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        
        return all_valid


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë°ì´í„° ê²½ë¡œ ì„¤ì • (ìœ ë‹ˆì½”ë“œ ì˜¤ë¥˜ ë°©ì§€)
    data_dir = "C:/Users/jmzxc/OneDrive/ë°”íƒ• í™”ë©´/ë¹…ì¹´ì¸ì¦ˆ"
    output_dir = "./data/processed"
    
    try:
        print("ğŸ“‹ ë¹…ì¹´ì¸ì¦ˆ ë‰´ìŠ¤ ê°ì„±ë¶„ì„ ì „ì²˜ë¦¬ ì‹œì‘")
        print(f"ğŸ“ ì…ë ¥ ê²½ë¡œ: {data_dir}")
        print(f"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {output_dir}")
        
        # ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        processor = BigKindsSentimentProcessor(data_dir, output_dir)
        
        # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
        saved_files = processor.process_all_files()
        
        if saved_files:
            # ì¶œë ¥ í˜•ì‹ ê²€ì¦
            processor.verify_output_format()
            
            print(f"\nğŸ”— ë‹¤ìŒ ë‹¨ê³„:")
            print(f"1. ìƒì„±ëœ CSV íŒŒì¼ë“¤ì´ ./data/processed/ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸")
            print(f"2. LSTM ë”¥ëŸ¬ë‹ ì½”ë“œ ì‹¤í–‰:")
            print(f"   python sentiment_deeplearning_analysis.py")
            
            return True
        else:
            print("âŒ ì²˜ë¦¬ ì‹¤íŒ¨")
            return False
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return False
        
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    
    if success:
        print("\n" + "="*50)
        print("ğŸŠ ê°ì„±ë¶„ì„ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("ğŸš€ ì´ì œ LSTM ë”¥ëŸ¬ë‹ ë¶„ì„ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
        print("="*50)
    else:
        print("\n" + "="*50)
        print("ğŸ’¥ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ“ í™•ì¸ì‚¬í•­:")
        print("1. ë¹…ì¹´ì¸ì¦ˆ ì—‘ì…€ íŒŒì¼ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
        print("2. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸")
        print("3. ì¸í„°ë„· ì—°ê²° ìƒíƒœ í™•ì¸ (FinBERT ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")
        print("="*50)


"""
ğŸ“‹ ì‚¬ìš©ë²•:

1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:
   pip install pandas numpy transformers torch tqdm openpyxl

2. ë¹…ì¹´ì¸ì¦ˆ íŒŒì¼ ì¤€ë¹„:
   - C:/Users/jmzxc/OneDrive/ë°”íƒ• í™”ë©´/ë¹…ì¹´ì¸ì¦ˆ/NewsResult_20210101-20211231.xlsx
   - C:/Users/jmzxc/OneDrive/ë°”íƒ• í™”ë©´/ë¹…ì¹´ì¸ì¦ˆ/NewsResult_20220101-20221231.xlsx  
   - C:/Users/jmzxc/OneDrive/ë°”íƒ• í™”ë©´/ë¹…ì¹´ì¸ì¦ˆ/NewsResult_20230101-20231231.xlsx
   - C:/Users/jmzxc/OneDrive/ë°”íƒ• í™”ë©´/ë¹…ì¹´ì¸ì¦ˆ/NewsResult_20240101-20241231.xlsx

3. ì½”ë“œ ì‹¤í–‰:
   python bigkinds_sentiment_processor.py

4. ê²°ê³¼ í™•ì¸:
   ./data/processed/ í´ë”ì— ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:
   - samsung_sentiment_2021.csv
   - samsung_sentiment_2022.csv
   - samsung_sentiment_2023.csv  
   - samsung_sentiment_2024.csv
   - apple_sentiment_2021.csv
   - apple_sentiment_2022.csv
   - apple_sentiment_2023.csv
   - apple_sentiment_2024.csv

ğŸ¯ ì¡°ì› ì½”ë“œì™€ì˜ ì™„ë²½í•œ í˜¸í™˜ì„±:
âœ… ë™ì¼í•œ FinBERT ëª¨ë¸ (snunlp/KR-FinBert-SC)
âœ… ë™ì¼í•œ ê°ì„±ì ìˆ˜ ë³€í™˜ ê³µì‹
âœ… ë™ì¼í•œ ê¸°ì—… ë¶„ë¥˜ ë¡œì§
âœ… ë™ì¼í•œ CSV ì¶œë ¥ í˜•ì‹
âœ… ë™ì¼í•œ ì»¬ëŸ¼ëª… ë° ë°ì´í„° êµ¬ì¡°

ğŸš€ ë‹¤ìŒ ë‹¨ê³„:
ì´ ì½”ë“œë¡œ ê°ì„±ë¶„ì„ì„ ì™„ë£Œí•œ í›„, 
sentiment_deeplearning_analysis.py ì½”ë“œë¡œ LSTM ì‹œê³„ì—´ ë¶„ì„ì„ ì§„í–‰í•˜ì„¸ìš”!
"""